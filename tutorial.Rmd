---
title             : "A Tutorial on Aggregating Evidence from Conceptual Replication Studies using the Product Bayes Factor"
shorttitle        : "PRODUCT BAYES FACTOR"

author:
  - name: "Caspar J. Van Lissa"
    affiliation: "1"
    corresponding: yes
    address: "Professor Cobbenhagenlaan 125, 5037 DB Tilburg, The Netherlands"
    email: "c.j.vanlissa@tilburguniversity.edu"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - Conceptualization
      - Formal Analysis
      - Funding acquisition
      - Methodology
      - Project administration
      - Software
      - Supervision
      - Writing – original draft
      - Writing – review & editing
  - name: "Eli-Boaz Clapper"
    affiliation: "2"
    role:
      - Formal Analysis
      - Writing – original draft
      - Writing – review & editing
  - name: "Rebecca Kuiper"
    affiliation: "2"
    role:
      - Conceptualization
      - Writing – review & editing
affiliation:
  - id            : "1"
    institution   : "Tilburg University, dept. Methodology & Statistics"
  - id            : "2"
    institution   : "Utrecht University, dept. Methodology & Statistics"

authornote: |
  This is a preprint paper, generated from Git Commit # `r substr(gert::git_commit_id(),1,8)`. This work was funded by a NWO Veni Grant (NWO Grant Number VI.Veni.191G.090), awarded to the lead author.

abstract: |
  The product Bayes factor (PBF) synthesizes evidence for an informative hypothesis across heterogeneous replication studies. It can be used when fixed- or random effects meta-analysis fall short. For example, when effect sizes are incomparable and cannot be pooled, or when studies diverge significantly in the populations, study designs, and measures used. PBF shines as a solution for small sample meta-analyses, where the number of between-study differences is often large relative to the number of studies, precluding the use of meta-regression to account for these differences. Users should be mindful of the fact that the PBF answers a qualitatively different research question than other evidence synthesis methods. For example, whereas fixed-effect meta-analysis estimates the size of a population effect, the PBF quantifies to what extent an informative hypothesis is supported in all included studies. This tutorial paper showcases the user-friendly PBF functionality within the `bain` R-package. This new implementation of an existing method was validated using an extensive simulation study, available in an Online Supplement. The simulation found that PBF had a high overall accuracy, due to greater sensitivity and lower specificity, compared to random-effects meta-analysis, individual participant data meta-analysis, and vote counting. This tutorial demonstrates applications of the method in several examples based on published research. The examples use datasets included in `bain`, so readers can reproduce the examples, or apply the code to their own data. The PBF is a promising method for synthesizing evidence for informative hypotheses across conceptual replications that are not suitable for conventional meta-analysis.
  
keywords          : "bayes factor, evidence synthesis, bayesian, meta-analysis"
wordcount         : "5039"

bibliography      : ["product_bayes.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
knit: worcs::cite_essential
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
run_everything <- TRUE
library("papaja")
library(tidySEM)
library(kableExtra)
library(bain)
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed, warning = FALSE)
library(targets)
tar_load_everything()
```

# Introduction

Recent years have seen a crisis of confidence over the reliability of published results in psychology, and science more broadly [@brembsPrestigiousScienceJournals2018].
This crisis has led to an increase in replication research as a way to derive knowledge that will stand the test of time [see @lavelleWhenCrisisBecomes2021]. 
In step with this interest in replication research,
research synthesis methods have become increasingly popular.
Such methods aggregate research findings,
and thus enable drawing overarching conclusions across multiple replication studies.
Meta-analysis is the most popular quantitative research synthesis method.
It estimates an overall population effect size based on several close replication studies.
One limitation is that its assumptions about the nature of between-studies heterogeneity may be untenable in some practical applications.
The Product Bayes Factor (PBF) is a valid alternative in such cases [@kuiperCombiningStatisticalEvidence2013].
Instead of estimating a pooled effect size across studies,
the PBF computes a Bayes factor that quantifies the amount of support the studies provide for a common informative hypothesis.
This tutorial introduces a novel user-friendly implementation of this existing method in the `bain` R-package.
Its performance was recently validated using a simulation study [@klugkistBayesianEvidenceSynthesis2023],
and the present study includes an Online Supplement with an additional simulation study to benchmark the present implementation against other commonly used research synthesis methods.
We demonstrate the use of the PBF in a series of reproducible Tutorial examples, based on re-analyis of data from published studies that previously used the PBF [@kuiperCombiningStatisticalEvidence2013; @vanleeuwenMoralityCooperationPolitics2024].

Differences between studies present a fundamental challenge to research synthesis methods [@higginsReevaluationRandomeffectsMetaanalysis2009].
Even when studies test the same hypothesis,
they often do so in different laboratories,
with idiosyncratic methods, and sample from distinct populations.
These between-study differences can introduce heterogeneity in the effect sizes found.
Meta-analysis is the most common quantitative research synthesis method.
It aggregates results of different studies by estimating an overall effect size [@borensteinIntroductionMetaAnalysis2009].
In meta-analysis, heterogeneity can be accounted for in four ways [see @vanlissaSmallSampleMetaanalyses2020].
First, if studies are exact replications, researchers might assume that they share a single true effect and estimate its value using fixed-effect meta-analysis.
Second, if heterogeneity between studies can be assumed to be normally distributed, random-effects meta-analysis can be used to estimate the mean and variance of this distribution.
This assumption holds, for example, when the effect size is influenced in a minor way by many uncorrelated factors.
Third, when there are a few moderators that cause larger systematic differences between studies,
their influence can be accounted for using meta-regression.
Note that it must be known beforehand which moderators are relevant,
and there must be sufficient studies relative to the number of moderators.
Fourth, when there are many moderators that could cause systematic differences, but it is not known beforehand which of them are relevant,
exploratory techniques like random forest meta-analysis and penalized meta-regression can be used to identify relevant moderators [@vanlissaSelectingRelevantModerators2023a].
Each of these approaches makes different assumptions about the nature of heterogeneity [see "Models for meta-analysis" in @vanlissaSmallSampleMetaanalyses2020].

A significant challenge arises in situations where none of the aforementioned assumptions are tenable.
This occurs when studies all assess the same informative hypothesis,
but differ in fundamental ways that preclude their effect sizes from being aggregated.
Consider, for example, the situation that arises when the number of moderators is large relative to the number of studies.
This issue is exemplified in the meta-analytic dataset from @kuiperCombiningStatisticalEvidence2013 in Table \@ref(tab:tabkuiper).
Each of the four studies is unique in its combination of moderator values,
making it impossible to control for their influence using meta-regression [@kuiperCombiningStatisticalEvidence2013].
The problem of multicollinear moderators is immediately evident in small datasets like this, but it also occurs in larger datasets, and may be less conspicuous [e.g., @kweeAnxiolyticEffectsEndocannabinoid2023].
Another example of fundamentally incomparable studies is the following:
Consider four studies which have sampled from the same populations, used the same study designs, the same measurement instruments, and the same statistical models.
At first glance, these studies might seem ideally suited for (fixed-effect) meta-analysis.
The situation changes, however, if each study controlled for a different set of covariates.
This would render the estimands of the effect size of interest fundamentally incomparable [@pearlSevenToolsCausal2019].
Another example occurs when transforming different effect sizes to a common metric (e.g., odds ratio to correlation coefficient).
While this is a common practice, researchers may be unaware that conversions often involve strong assumptions and may result in estimates with unknown distributional properties [@vanassenEndJustifiesAll2023].
These examples illustrate the general problem that it may not always be possible to estimate a pooled effect size across studies.
In such cases, the PBF can still be used to determine whether all studies support a common informative hypothesis.

```{r tabkuiper, eval = TRUE, echo = FALSE}
library(bain)
tab_kuiper <- bain::kuiper2013
tab_kuiper$Type <- c("Survey", "Experiment", "Experiment", "Experiment")
tab_kuiper$Sample <- c("Managers", "Managers", "Students", "Students")
tab_kuiper$Analysis <- c("Linear regression", "Linear regression", "Probit regression", "Multilevel model")
papaja::apa_table(tab_kuiper, digits = 2, caption = "Data from Kuiper et al., 2013.")
```


```{r tabmulticol, eval = TRUE, echo = FALSE, results = 'asis'}
library(metafor)
dat <- escalc(measure="RD", n1i=n1i, n2i=n2i, ai=ai, ci=ci, data=dat.hine1989)
dat <- dat[sample(nrow(dat), 4), ]
tab <- data.frame(Study = 1:4,
                  Sample = c("Europe", "Europe", "USA", "China"),
                  Design = c("Experiment", "Survey", "Experiment", "Survey"),
                  Outcome = c("Reaction time", "Self-report", "Self-report", "Reaction time"),
                  yi = dat$yi,
                  vi = dat$vi)
papaja::apa_table(tab, caption = "Example of incomparabe studies", digits = 2)
```

### What are Informative Hypotheses?

Informative hypotheses relate the parameters of interest (e.g., $\beta$) to each other and/or to numerical constants by means of (in)equality constraints. The conventional null hypothesis $H_0$ can be thought of as a special case of an informative hypothesis, which specifies that the parameter is equal to the value zero, $H_i: \beta = 0$. But informative hypotheses can be much more complex, and ideally, should closely represent the theoretical expectation. If one is confident that a new drug  one might hypothesize that a parameter is larger than a smallest effect size of interest, $H_i: \beta > 0.1$ [see @lakensEquivalenceTestsPractical2017]. If a dose-response effect is expected for a certain drug, one might hypothesize that three treatments of increasing intensity will have effects of increasing size, and that all these treatments will exceed the smallest effect size of interest, $H_i: (\beta_1 < \beta_2 < \beta_3) > 0.1$.

### Bayesian Informative Hypothesis Tests

The amount of evidence for an informative hypothesis $H_i$ relative to another hypothesis can be expressed as a Bayes factor, or BF.
Bayes factors for one hypothesis are thus always defined in relation to another hypothesis.
In this tutorial paper, all informative hypotheses $H_i$ are compared to their complement $H_{!i}$ [see @guApproximatedAdjustedFractional2018].
This complement $H_{!i}$ can be interpreted as "not $H_i$".
This Bayes factor, which we will refer to as $BF_c$, represents the ratio of evidence for $H_i$ relative to the evidence against it.
A value of $BF_c = 10 = \frac{10}{1}$ means that the data provide ten times more support for the hypothesis than against it;
a value of $BF_c = 0.1= \frac{1}{10}$ means that the data provide ten times less support for the hypothesis than against it.

Testing hypotheses using the Bayes factor deviates in important ways from conventional null-hypothesis significance testing.
It involves thinking about probability in a more subjective way, which allows one to express confidence in the truth of a hypothesis.
This deviates from frequentist thinking, where probability is defined in objective terms, as a function of the frequency with which an event would occur if a procedure would be repeated many times.
As a result of these differences in interpretation, Bayes factors provide a continuous measure of evidence for the informative hypothesis, rather than a binary decision to (not) reject a typically uninformative null hypothesis.
Bayes factors can also be used to evaluate relative support for competing informative hypotheses.
Moreover, they are robust to multiple- and sequential testing.
The Bayes factor approach also has downsides; most notably, the absence of hard-and-fast decision criteria [although some have suggested cut-offs of $BF > 3$ or $BF > 10$ as conclusive evidence, @rouderBayesianTestsAccepting2009].
Importantly, the Bayes factor is not inherently better or worse than null-hypothesis significance testing.
It is just an alternative inferential procedure, which - under some conditions - performs similarly to null-hypothesis significance tests [@mulderPriorAdjustedDefault2014].

### How are Bayes Factors Calculated?

This tutorial uses the *Approximated adjusted fractional Bayes factor* (AAFBF) first introduced by @mulderPriorAdjustedDefault2014.
Conceptually, AAFBFs are computed by integrating so-called posterior and prior distributions with respect to the parameter constraints specified in the hypothesis, and taking the ratio of these quantities.
The term *approximated* in AAFBF refers to the fact that the prior and posterior are both approximated by normal distributions,
which makes it possible to compute these Bayes factors from sufficient statistics.
The (multivariate) normal posterior is centered around the maximum likelihood estimate of the parameter(s),
and its its covariance consists of the asymptotic (co)variance matrix of the parameters.
For a single parameter, this simplifies to the parameter estimate and its sampling variance (the squared standard error).
The term *adjusted* in AAFBF refers to the fact that the center of the prior distribution is adjusted based on the set of informative hypotheses,
and term *fractional* refers to the fact that the the prior covariance matrix is based on a fraction of the information contained in the posterior covariance matrix.
Computation is simplified by sampling random values from these normal distributions instead of integrating over them analytically.
This introduces slight random fluctuations in the results.
To make the analyses reproducible, it is therefore important to set a "random seed",
which ensures that the same pseudo-random numbers will be generated upon repeated evaluation of the code.
This is done throughout the tutorial examples.
For a more detailed technical description of the AAFBF, readers can turn to @guApproximatedAdjustedFractional2018; for mathematical derivations, see @mulderPriorAdjustedDefault2014.

### The Product Bayes Factor

The PBF is a Bayesian evidence synthesis (BES) technique that aggregates the evidence for a theoretical relationship across studies,
without imposing assumptions about heterogeneity.
As explained in @kuiperCombiningStatisticalEvidence2013, aggregating evidence across studies comes down to taking the product of Bayes factors from these individual studies.
Note that Bayes factors are always a ratio of evidence in favor of one hypothesis over another hypothesis.
The PBF assumes that these hypotheses are a priori equally likely.

When multiple studies each provide evidence for $H_i$ in the form of complement Bayes factors,
these Bayes factors can be synthesized across studies by taking their product [@kuiperCombiningStatisticalEvidence2013].
The resulting product Bayes factor (PBF) summarizes the total evidence for the hypothesis.
The only assumption of the PBF is that all study-specific hypotheses provide evidence about the same underlying theoretical relationship.
Note that other approaches to BES exist;
for instance, it is possible to use the posterior of one study as the prior for a replication study, and thus accumulate evidence across studies [see @heckReviewApplicationsBayes2022].
Such applications are out of scope of the present paper,
which addresses the PBF approach to BES.

Although meta-analysis and PBF are both research synthesis methods, they answer different research questions.
Meta-analysis estimates the point estimate or distribution of a population effect size. 
It pools estimates of this effect size across multiple studies to obtain an overall estimate of the effect size.
If all assumptions are met, this is a consistent estimate of the population effect size.
It thus answers questions like:
Given certain assumptions about between-studies heterogeneity, what is the average population effect size?
The PBF, on the other hand, aggregates evidence for an informative hypothesis across multiple studies.
It thus answers the question: Do all studies support the hypothesis of interest?
The PBF is emphatically *not* a consistent estimator of the amount of support the hypothesis would receive based on individual participant data.
Both methods thus answer different research questions,
and provide complementary information.

### Are Studies' Sample Sizes Taken into Account?

Conventional meta-analysis can be conceptualized as a weighted average of study effect sizes [@borensteinIntroductionMetaAnalysis2009].
Each study's sample size indirectly affects its weight, via its influence on the sampling variance (the squared standard error).
The approximate fractional Bayes factors also accounts for sample size to some extent,
as the posterior (co)variance used to calculate the Bayes factors is the sampling variance of the estimate,
just as in meta-analysis.
The prior distribution is also based, to some extent, on the sample size [@hoijtinkTutorialTestingHypotheses2019].
The resulting Bayes factor for a true hypothesis increases monotonically with increasing sample size.
Thus, the overall support for a true hypothesis will be greater if it is based on larger, rather than smaller, samples.

## Simulation Study

This tutorial paper introduces a new implementation of the existing PBF method.
A simulation study validating this new implementation is available in an Online Supplement, and the reproducible code is available at [MASKED FOR REVIEW].
We simulated a PBF analysis of correlation coefficients and manipulated the presence or absence of a true population effect, the sample size per study, the number of replication studies, and the the reliability of the correlated variables. We compared the performance of the PBF against vote counting [@hedgesVotecountingMethodsResearch1980], random-effects meta-analysis [@viechtbauerConductingMetaanalysesMetafor2010], and individual participant data meta-analysis [@rileyMetaanalysisIndividualParticipant2010].
PBF had the highest overall accuracy, primarily due to its greater sensitivity to detecting a true effect.
However, PBF had lower specificity than all other algorithms, suggesting a trade-off between sensitivity and specificity.
The other algorithms showed ceiling effects in specificity, limiting their sensitivity.
The performance of the PBF was most strongly affected by sample size,
followed by the number of samples and reliability.

## This Tutorial Paper

This paper introduces the first implementation of the PBF in user-friendly free open source software.
This tutorial requires the `bain` R-package for Bayesian informative hypothesis evaluation, version `0.2.11`, which contains the `pbf()` function and all tutorial data.
The Online Supplement presents a simulation study that validates our implementation of the PBF and benchmark it against alternative evidence synthesis methods, including random-effects restricted maximum likelihood meta-analysis, individual participant data meta-analysis, and vote counting.
This paper focuses on applications of the PBF, illustrated by several reproducible examples.
This tutorial was, itself, made reproducible using the Workflow for Open Reproducible Code in Science (WORCS).
The code archive is available at <https://github.com/cjvanlissa/bayesynth>.

# Tutorial

This tutorial demonstrates how to synthesize evidence for an informative hypothesis across heterogeneous replications using the Product Bayes Factor (PBF). 
We assume that users have installed the free open source statistical programming language R [@rcoreteamLanguageEnvironmentStatistical2022].
The R-package `bain` can be installed by running `install.packages("bain")` in the R console.
The datasets used in this tutorial are all included in the `bain` package.
The `kuiper2013` dataset is based on @kuiperCombiningStatisticalEvidence2013 and the original publications meta-analyzed therein.
Its documentation is accessed by running `?kuiper2013` in the R console.
The other datasets were simulated based on data presented in [@vanleeuwenMoralityCooperationPolitics2024].
Their documentation is accessed by running `?synthetic_us`, `?synthetic_dk` or `?synthetic_nl`.
While we introduce the basic functionality of the `bain` package,
we direct the interested reader to @hoijtinkTutorialTestingHypotheses2019 for a general introduction to `bain`,
and to @vanlissaTeacherCornerEvaluating2020 for an in-depth tutorial on informative hypothesis tests for Structural Equation Models using `bain`.

## Tutorial 1: When Meta-Analysis Falls Short

The PBF was first introduced by @kuiperCombiningStatisticalEvidence2013
for cases where random-effects meta-analysis would otherwise be used,
but is deemed inappropriate because its assumptions are likely to be violated.
This Tutorial illustrates the use of the PBF in such cases,
and compares it to meta-analysis to show that one can straightforwardly perform a PBF analysis with a dataset prepared for meta-analysis.
As explained in the introduction, if the hypothesis of interest pertains to only one parameter, then the Bayes factor can be computed using the estimate and its standard error [@hoijtinkTutorialTestingHypotheses2019].
It is also possible to formulate more complex hypotheses that involve multiple parameters; for example, in structural equation models [see @vanlissaTeacherCornerEvaluating2020].
This requires access to the parameter covariance matrix.
The present example focuses on sufficient statistics to illustrate how one might complement a conventional meta-analysis with a PBF analysis and report both in the same paper.
This way, readers can determine whether they trust the assumption of normally distributed heterogeneity and interpret the random-effects estimate,
or doubt the assumption and interpret the PBF instead.

The data for this example is shown in Table \@ref(tab:tabkuiper).
Run `?kuiper2013` to view its documentation.
Kuiper and colleagues [-@kuiperCombiningStatisticalEvidence2013] set out to aggregate evidence for the effect of prior interactions between partners on trust in (economic) exchange relations across four heterogeneous replication studies.
All studies investigated the informative hypothesis that past (experience) with a seller has a positive effect on trust.
Batenburg et al. (2003) analyzed survey data using linear regression with covariates; Buskens and Raub (2002) analyzed experimental data using linear regression; Buskens and Weesie (2000) used an experimental design with a binary outcome, analyzed using probit regression; and Buskens, Raub, and Van der Veer (2010) used a longitudinal experimental design, analyzing the data with a three-level logistic regression.
These studies each provide a regression coefficient assessing the effect of past experience on trust, and its estimated sampling variance (squared standard error).
However, because the studies differ in terms of design (survey vs. experiment),
operationalization of variables, measurement level of variables, and statistical model used,
these regression coefficients are not directly comparable and should not be pooled using meta-analysis.
The authors developed the PBF to determine whether all studies support the informative hypothesis.

Although there is one clear informative hypothesis, we follow the original study and estimate the evidence for three competing hypotheses: $H_1: \beta = 0$ (the null hypothesis that the effect of past on trust is zero), $H_2: \beta > 0$ (a directional hypothesis that the effect of past on trust is positive), and $H_3: \beta < 0$ (a directional hypothesis that the effect of past on trust is negative).
Note that Kuiper and colleagues computed the Bayes factors and posterior model probabilities by hand, using custom priors for the first study, and using the posterior of the first study as prior for subsequent studies.
The present tutorial instead uses approximate fractional Bayes factors computed using `bain`, with default settings. 
The prior for these Bayes factors is derived from the data.
Consequently, the numerical results differ from Kuiper and colleagues, although the conclusions remain the same.

We first conduct a random-effects meta-analysis using the function `rma()` from the `metafor` package.
Then, we perform a PBF analysis using the `pbf()` function in the `bain` package.
This allows us to compare the interface of both functions and their results.
First, we will load the required packages.
This also makes the `kuiper2013` available.
Printing it to the console should give the same result as Table \@ref(tab:tabkuiper).

```{r echo = TRUE, eval = TRUE, results='hide'}
library(metafor)
library(bain)
kuiper2013
```

To perform a random-effects meta-analysis, run the following code:

```{r echo = TRUE, eval = FALSE}
rma(yi = kuiper2013$beta, vi = kuiper2013$vi)
```

```{r echo = FALSE, eval = TRUE}
out <- capture.output(print(res_rma, digits = 2))
out <- out[c(2:4, 8:15)]
out <- out[!out == ""]
cat(out, sep = "\n")
```

Note that the pooled effect size is $\beta `r report(res_rma[["b"]][1,1])`$.
Considering our hypothesis is one-sided, we can divide the p-value by two, and report $p `r report(res_rma[["pval"]]/2)`$.
Thus, there is a significant positive effect of prior interactions on trust.
However, this analysis assumes a normal distribution of population effect sizes.
We doubt this assumption, because the studies are all qualitatively different.
While empirical evidence for this violation of assumptions is given by a significant heterogeneity test,
we do not need to make a purely data-driven decision [@wichertsDegreesFreedomPlanning2016].
We can support the assumption that population effect sizes are *not* normally distributed on theoretical grounds.

We now perform the PBF analysis, using the `pbf()` method for numeric input (see `?pbf`).
This interface is very similar to `rma()`, and is specifically designed for applications where PBF is applied to meta-analytic datasets.
Because PBF relies on Monte Carlo estimation (i.e., randomly sampling values),
however, it is advisable to set a seed to make the analysis reproducible.
Throughout this tutorial we use the value `set.seed(1)`,
but in your analyses, make sure to select a different unique value.
Run the following code:

```{r echo = TRUE, eval = FALSE}
set.seed(1)
pbf(yi = kuiper2013$beta,
    vi = kuiper2013$vi,
    ni = kuiper2013$n,
    hypothesis = "y = 0; y > 0; y < 0")
```
```{r echo = FALSE, eval = TRUE}
print_pbf <- function(x){
  out <- x
  out[,] <- sapply(x, formatC, digits = 2, format = "f")
  if(any(nchar(unlist(out)) > 8)){
    for(i in 1:nrow(out)){
      for(j in 1:ncol(out)){
        if(nchar(out[i,j]) > 8){
          out[i,j] <- formatC(x[i,j], format = "e", digits = 2)
        }
      }
    }
  }
  print(out)
}
print_pbf(res_pbf_t1)
```

Note that the `yi` and `vi` arguments are the same as those of `rma()`.
Additional argument `ni` is used to construct the prior for the approximate Bayes factors [@hoijtinkTutorialTestingHypotheses2019].
Importantly, the `hypothesis` argument determines which informative hypotheses are tested.
Its default value of `y = 0` is similar to a null hypothesis test.
Here, we override this default value to test our three informative hypotheses.
The resulting output shows Bayes factors for each of the three hypotheses (PBF column), as well as the study-specific evidence for each hypothesis (remaining columns).
Note that the PBF for hypotheses $H_1$ and $H_3$ are approximately zero; there is essentially no support in the data that the effect of prior interaction on trust is equal to, or smaller than, zero.
Support for $H_2$ is overwhelming, however.
Thus, we can conclude that all four included studies support the informative hypothesis that there is a positive effect of prior interaction on trust.

## Tutorial 2: Computing a Bayes Factor

The preceding example used a simplified interface to the PBF, designed to be similar to `rma()` to facilitate analyzing data initially prepared for meta-analysis.
The simplified interface internally performs several intermediate steps.
These next tutorials go through those steps one by one, to help users understand the calculations involved.
As these calculations require individual participant data,
we cannot use the sufficient statistics from Tutorial 1.
For these tutorials we use the synthetic data based on
Van Leeuwen and colleagues [@vanleeuwenMoralityCooperationPolitics2024].
They conducted a theory-driven, preregistered study of the informative hypothesis that higher self-reported moral dispositions would be associated with a more conservative socio-political orientation.
Data were collected in three countries: the United states of America, Denmark, and the Netherlands.
Each sample contained multiple measures of political orientation and moral dispositions.
In the original publication, the PBF was used to aggregate evidence across scales and countries to obtain an overall measure of support for the informative hypothesis.
This tutorial follows the same rationale, but uses only one effect size per sample.
We intentionally vary the way this effect size is computed to illustrate the use of the PBF in cases when the same informative hypothesis has been studied in different ways in multiple studies.
Specifically, we will examine the informative hypothesis that self-reported importance of family morality is positively associated with a conservative socio-political orientation.

We must estimate a model suitable for evaluating this informative hypothesis.
Because both scales consist of multiple items, we can use structural equation modeling (SEM) to perform latent variable regression [see @vanlissaTeacherCornerEvaluating2020]:

```{r echo = TRUE, eval = FALSE}
# Load lavaan package for SEM
library(lavaan)

# Specify SEM-model for latent variable regression
model_nl <- "
fam =~ fam_1 + fam_2 + fam_3
con =~ sepa_soc_1 + sepa_soc_2 + sepa_soc_3 + sepa_soc_4 + sepa_soc_5 +
       sepa_eco_1 + sepa_eco_2 + sepa_eco_3 + sepa_eco_4 + sepa_eco_5
con ~ beta * fam"

# Estimate the model in lavaan
results_nl <- sem(model = model_nl, data = synthetic_nl)
```

Whereas the previous Tutorial used a test value of zero, similar to conventional frequentist null hypothesis testing, this tutorial uses a smallest effect size of interest [@lakensEquivalenceTestsPractical2017].
Thus, our informative hypothesis is $H_i: \beta > .1$, where $\beta$ (beta) is the standardized (partial) regression coefficient, and we hypothesize that its value will be at least $.1$.

The code below illustrates how to use the `bain()` function to obtain a Bayes factor for this informative hypothesis.
In the `hypothesis` argument, we can refer to the parameter `beta` by name because we labeled it in the `lavaan` syntax above.
If we had not done labeled the parameter, we could inspect all parameter names and their values by running `get_estimates(results_nl, standardize = TRUE)`.

```{r echo = TRUE, eval = FALSE}
# Test that the effect labeled 'beta' is positive
set.seed(1)
bf_nl <- bain(results_nl,
              hypothesis = "beta > .1",
              standardize = TRUE)
bf_nl
```

```{r echo = FALSE, eval = TRUE}
print_bain <- function(x, stats = c("Fit", "Com", "BF.u", "BF.c","PMPa", "PMPb"),
                       digits = 3,
                       na.print = "", ...){


  fits <- as.matrix(x$fit)

  if (dim(fits)[2] == 12){stats[7] <- "PMPc"}

  dat <- fits[, stats]
  miss_val <- is.na(dat)
  dat <- formatC(dat, digits = digits, format = "f")
  dat[miss_val] <- ""
  model_type <- class(x$model)[1]
  if(model_type == "lm"){
    model_type <- paste0(model_type, " (", attr(x, "which_model"), ")")
  }
  cat("Bayesian informative hypothesis testing for an object of class ", model_type, ":\n\n", sep = "")

  prmatrix(dat,
           quote = FALSE,
           na.print = na.print)


  if (dim(fits)[2] == 12){
  cat("\nHypotheses:\n ", paste(rownames(dat)[c(-nrow(dat),-(nrow(dat)-1))], ": ", x$hypotheses, sep = "", collapse = "\n  "))}
  if (dim(fits)[2] == 11){
    cat("\nHypotheses:\n ", paste(rownames(dat)[-nrow(dat)], ": ", x$hypotheses, sep = "", collapse = "\n  "))}

}
print_bain(res_t2, digits = 2)
```

The results indicate that the informative hypothesis receives about 23 times as much support from the data relative to its complement, which is convincing evidence.

## Tutorial 3: Aggregating Bayes Factors

As mentioned before, suitable data were collected to evaluate the informative hypothesis in three countries.
There are differences between countries that prevent analyzing these data as a multilevel model, however.
For instance, conservatism was measured using different scales.
This is an appropriate situation to use the PBF to aggregate evidence across countries.
Below, we estimate a latent regression model for the remaining two countries, taking care to use the same label for the parameter of interest in all samples.
Then, we bind all three SEM-models in a list, and call PBF to evaluate the hypothesis of interest on all models and aggregate the evidence.
As the BF in all three samples is positive, the resulting PBF is very large.
We can thus conclude that the central hypothesis receives overwhelming support across samples.

```{r, eval = FALSE, echo = TRUE}
# Specify the models for synthetic_dk and synthetic_us
model_dk <- "
fam =~ fam_1 + fam_2 + fam_3
con =~
sepa_soc_1 + sepa_soc_2 + sepa_soc_3 + sepa_soc_4 + sepa_soc_5 +
sepa_eco_1 + sepa_eco_2 + sepa_eco_3 + sepa_eco_4 + sepa_eco_5
con ~ beta * fam"
model_us <- "
fam =~ fam_1 + fam_2 + fam_3
con =~
secs_soc_1 + secs_soc_2 + secs_soc_3 + secs_soc_4 + secs_soc_5 +
secs_soc_6 + secs_soc_7 +
secs_eco_1 + secs_eco_2 + secs_eco_3 + secs_eco_4 + secs_eco_5
con ~ beta * fam"

# Estimate the model in lavaan
results_dk <- sem(model = model_dk, data = synthetic_dk)
results_us <- sem(model = model_us, data = synthetic_us)

# Bind the models into a list
results <- list(results_nl, results_dk, results_us)
# Test the hypothesis that the effect size labeled 'beta' is positive
set.seed(1)
pbf(results, hypothesis = "beta > .1", standardize = TRUE)
```
```{r, eval = TRUE, echo = FALSE}
print_pbf(res_pbf_t3)
```

## Tutorial 4: Using `bain` Objects

The `pbf()` function also accepts multiple `bain` objects.
This makes it possible to, for example, evaluate different sets of hypotheses on different data sets before using the resulting `bain` objects to aggregate the evidence for all common hypotheses across datasets.
The example below illustrates this use case.
As before, all analyses share one hypotheses in common ($H_i: \beta_{fam} > .1$),
but the Dutch sample now contains a sample-specific hypothesis regarding the effect of group morality, namely that $\beta_{grp} < .1$.
Note that controlling for the effect of group morality affects the effect of family morality on conservatism, which is now a partial effect.
This also affects the Bayes factor for this sample.
The `pbf()` function is called on a list of `bain` objects.
Note that, in this case, `pbf()` does not require an argument `hypothesis`, as the hypotheses are contained in the individual `bain` objects.

```{r, echo = TRUE, eval = FALSE}
# Add the additional predictor to the model, label the effect beta2
model_nl2 <- c(model_nl, "group =~ grp_1 + grp_2 + grp_3
                         con ~ beta2 * group")

# Estimate the model in lavaan
results_nl2 <- sem(model = model_nl2, data = synthetic_nl)

# Obtain BF for each sample
# Note that the Dutch sample has two hypotheses:
set.seed(1)
bf_nl2 <- bain(results_nl2,
               hypothesis = "beta > .1; beta2 < .1", 
               standardize = TRUE)
bf_dk <- bain(results_dk, hypothesis = "beta > .1", standardize = TRUE)
bf_us <- bain(results_us, hypothesis = "beta > .1", standardize = TRUE)
```

```{r, echo = TRUE, eval = FALSE}
# Bind bain objects into a list
bfs <- list(bf_nl2, bf_dk, bf_us)

# Call pbf on that list
pbf(bfs)
```

```{r, echo = FALSE, eval = TRUE}
print_pbf(res_pbf4)
```

In the output of this analysis, hypotheses common to all `bain` objects are retained and aggregated, but the sample-specific hypothesis of the first object is omitted.
If there are no common hypotheses across all objects, `pbf()` throws an error.
As can be seen, the results are somewhat different due to the additional control variable - but the conclusions are equivalent to the previous tutorial.
Note the location of the `set.seed()` command: it is called before the initial `bain()` call,
because the `bain()` function relies on random sampling - but `pbf()` does not.
Compare this to Tutorial 3, where we called `set.seed()` before `pbf()`.
In that case, `pbf()` called the `bain()` function internally.


## Tutorial 5: Using Sufficient Statistics

Bringing our examples full circle, we once again illustrate how to compute the PBF from sufficient statistics - but in this case, we will calculate them ourselves instead of extracting them from published papers as in Tutorial 1.
In this Tutorial, we use the default interface of `bain`, as explained in [@hoijtinkTutorialTestingHypotheses2019].
This function requires four arguments: A named vector of parameter estimates, their asymptotic covariance matrix, the original sample size, and the number of within-group and between-group parameters.
Note that, when analyzing a single parameter per sample, the standard error is sufficient to construct the asymptotic covariance matrix.

The present use case evaluates the following hypothesis:
*There is a positive association between family morality and political conservatism*.
This conceptual hypothesis is evaluated differently in the three samples, resulting in three different types of statistics and distinct sample-specific hypotheses:

1. A t-test was performed using the synthetic_nl data; using Cohen's D gives $H_i^{synthetic_nl}: \delta_{conservative > liberal} > 0$, where $\delta$ is the mean difference between groups.
1. A bivariate regression coefficient was calculated using the synthetic_dk data, giving $H_i^{synthetic_dk}: \beta_{fam} > 0$
1. A correlation coefficient was calculated using the synthetic_us data, giving $H_i^{synthetic_us}: \rho_{fam,con} > 0$, where $\rho$ is the correlation between family morality and conservatism.

Note that we intentionally manipulate the data to illustrate these different analyses;
for example, we compute mean scale scores and dichotomize the continuous conservatism scale to conduct a t-test.
We do not advocate these practices for applied research.

First we obtain the relevant parameter estimates and their sampling variances,
which allows us to evaluate the specific hypotheses in `bain`:

```{r eval = FALSE, echo = TRUE}
# Create mean scale scores
synthetic_nl <- data.frame(
  family = rowMeans(synthetic_nl[c("fam_1", "fam_2", "fam_3")]),
  conservative = rowMeans(synthetic_nl[c("sepa_soc_1", "sepa_soc_2", "sepa_soc_3",
                               "sepa_soc_4", "sepa_soc_5", "sepa_eco_1",
                               "sepa_eco_2", "sepa_eco_3", "sepa_eco_4",
                               "sepa_eco_5")]))
synthetic_dk <- data.frame(
  family = rowMeans(synthetic_dk[c("fam_1", "fam_2", "fam_3")]),
  conservative = rowMeans(synthetic_dk[c("sepa_soc_1", "sepa_soc_2", "sepa_soc_3",
                               "sepa_soc_4", "sepa_soc_5", "sepa_eco_1",
                               "sepa_eco_2", "sepa_eco_3", "sepa_eco_4",
                               "sepa_eco_5")]))

synthetic_us <- data.frame(
  family = rowMeans(synthetic_us[c("fam_1", "fam_2", "fam_3")]),
  conservative = rowMeans(synthetic_us[c("secs_soc_1", "secs_soc_2", "secs_soc_3",
                               "secs_soc_4", "secs_soc_5", "secs_soc_6",
                               "secs_soc_7", "secs_eco_1", "secs_eco_2",
                               "secs_eco_3", "secs_eco_4", "secs_eco_5")]))

# synthetic_nl: Conduct t-test using Cohen's D
synthetic_nl$group <- cut(synthetic_nl$conservative, breaks = 2,
                labels = c("liberal", "conservative"))
sample_sizes <- table(synthetic_nl$group)
sds <- tapply(synthetic_nl$family, synthetic_nl$group, sd)
pooled_sd <- sqrt(sum((sample_sizes - 1) * sds) / (sum(sample_sizes) - 2))
NL_est <- diff(tapply(synthetic_nl$family, synthetic_nl$group, mean)) / pooled_sd
NL_var <- (sum(sample_sizes) / prod(sample_sizes)) +
  (NL_est^2 / (2*sum(sample_sizes)))

# synthetic_dk: Conduct bivariate regression
DK_fit <- lm(conservative ~ family, data = synthetic_dk)
DK_est <- coef(DK_fit)["family"]
DK_var <- vcov(DK_fit)["family", "family"]

# synthetic_us: Correlation coefficient
US_est <- cor(synthetic_us)[1, 2]
US_var <- (1 - US_est^2)^2 / (nrow(synthetic_us) - 1)

# Name the estimates so hypotheses will be the same
names(NL_est) <- names(DK_est) <- names(US_est) <- "parameter"
```

Then, we use `bain.default()` to evaluate the informative hypothesis on each parameter estimate. 
The `pbf()` function can be called on a list of the resulting bain objects.

```{r eval = FALSE, echo = TRUE}
# Use bain.default() to obtain BF for the central hypothesis 
NL_bain <- bain(x = NL_est, 
                Sigma = matrix(NL_var, 1, 1),
                n = nrow(synthetic_nl),
                hypothesis = "parameter > 0",
                joint_parameters = 1)
DK_bain <- bain(x = DK_est,
                Sigma = matrix(DK_var, 1, 1),
                n = nrow(synthetic_dk),
                hypothesis = "parameter > 0",
                joint_parameters = 1)
US_bain <- bain(x = US_est,
                Sigma = matrix(US_var, 1, 1),
                n = nrow(synthetic_us),
                hypothesis = "parameter > 0",
                joint_parameters = 1)

# Aggregate evidence using pbf()
pbf(list(US_bain, DK_bain, NL_bain))
```

```{r echo = FALSE, eval = TRUE}
print_pbf(res_pbf_t5)
```

The results suggest substantial evidence for the hypothesis that there is a positive association between family morality and political conservatism.
Although each study used a different method to assess this hypothesis,
their evidence can be synthesized using `pbf()`.

# Conclusion

<!-- Researchers need to choose a method that best suits their research question, -->
 <!-- the study highlights the importance of carefully considering the research question when selecting a method for meta-analysis, and the potential trade-offs that may exist between sensitivity and specificity when using different methods. -->

The Product Bayes Factor (PBF) is a suitable solution when aggregating evidence across conceptual replication studies that are too heterogeneous to meet the assumptions of conventional meta-analysis.
In contrast to meta-analysis, the PBF does not estimate the value of an overall effect size and its heterogeneity.
Instead, the PBF quantifies support for a common informative hypothesis across studies.
While this may be perceived as a loss of information, the PBF is used in situations where the assumptions of conventional meta-analysis are violated, and consequently, such estimates would not be informative.

One important distinction between meta-analysis and PBF is that the former provides ever-more precise estimates of the overall effect size as more studies are added.
There is no straightforward parallel for the PBF.
If all aggregated studies support the informative hypothesis, the PBF will increase as more studies are added.
However, if the studies offer mixed evidence of the informative hypothesis, the PBF can be inconclusive, even for a large number of studies.
This is because the PBF essentially answers the question:
do these studies all support the informative hypothesis?
If the answer is no, the results will reflect that.

We introduced a user-friendly implementation of the PBF in the `bain` R-package,
and demonstrated its use with various analysis techniques in R,
as well as with sufficient statistics that are already routinely coded for meta-analysis (i.e., effect sizes and their sampling variance).
This means that researchers can now use the PBF to aggregate evidence in situations where classic meta-analytic methods are less suitable.
For example, when one informative hypothesis has been evaluated in several replication studies,
but these replication studies are quite heterogeneous because they sample from different populations and use different methods or analysis techniques.
Especially when the number of replication studies is too small to adequately account for these sources of between-study heterogeneity,
the PBF may be a useful method to aggregate evidence for the common informative hypothesis.
Researchers should be aware that the PBF trades off increased sensitivity for decreased specificity,
and that it addresses a different research question than other research synthesis methods.
This highlights the importance of careful interpretation of the results,
and consideration of the research question when selecting an aggregation method.
In sum,
our results suggest that PBF is a useful evidence synthesis method,
which is now broadly accessible due to its inclusion in the `bain` R-package.

\newpage
# Highlights

* Many research synthesis methods make strong assumptions about between-studies heterogeneity that are violated when studies are conceptually replicated.
* The product Bayes factor (PBF) aggregates evidence for an informative hypothesis across conceptual replication studies without imposing assumptions about heterogeneity.
* This paper introduces a user-friendly way to compute the PBF for a variety of widely used models via the `pbf()` function in the `bain` R-package.
* A simulation study shows favorable performance for PBF relative to random effects meta-analysis, individual participant data meta-analysis, and vote counting.
* Three tutorial examples illustrate distinct use cases of the method.

# Data Availability Statement

All analysis code is available in a version-controlled repository at <https://github.com/cjvanlissa/bayesynth>.

# Conflict of Interest Statement

The authors declare no conflict of interest.


\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup













































