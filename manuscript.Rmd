---
title             : "Tutorial: Aggregate evidence from heterogeneous replication studies using the product Bayes factor"
shorttitle        : "PRODUCT BAYES FACTOR"

author:
  - name: "Caspar J. Van Lissa"
    affiliation: "1,2"
    corresponding: yes
    address: "Padualaan 14, 3584CH Utrecht, The Netherlands"
    email: "c.j.vanlissa@uu.nl"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - Conceptualization
      - Formal Analysis
      - Funding acquisition
      - Methodology
      - Project administration
      - Software
      - Supervision
      - Writing – original draft
      - Writing – review & editing
  - name: "Eli-Boaz Clapper"
    affiliation: "1"
    role:
      - Formal Analysis
      - Writing – original draft
      - Writing – review & editing
affiliation:
  - id            : "1"
    institution   : "Utrecht University, dept. Methodology & Statistics"
  - id            : "2"
    institution   : "Open Science Community Utrecht"

authornote: |
  This is a preprint paper, generated from Git Commit # `r substr(gert::git_commit_id(),1,7)`. This work was funded by a NWO Veni Grant (NWO Grant Number VI.Veni.191G.090), awarded to the lead author.

abstract: |
  TODO
  
keywords          : "bayes factor, evidence synthesis, bayesian, meta-analysis"
wordcount         : "5356"

bibliography      : ["product_bayes.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
knit: worcs::cite_essential
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
run_everything <- FALSE
library("papaja")
library(tidySEM)
library(kableExtra)
library(pwr)
r_refs("r-references.bib")
out <- readRDS("out.RData")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed, warning = FALSE)
```

Recent years have seen a crisis of confidence over the reliability of published results in psychology, and science more broadly [@brembs2018prestigious].
Replication research has come into focus as a solution to this crisis and as a way to derive knowledge that will stand the test of time [see @lavelleWhenCrisisBecomes2021]. 
<!--For many, highly regarded published studies however, results could not be replicated [@brembs2018prestigious].-->
<!--A lack of validating replication research partly strengthened the belief that single studies results should solely be viewed as part of many studies studying the same research question [@asendorpf2016recommendations]. -->
<!--Evidence over multiple studies must be aggregated to derive more reliable conclusions and within this paradigm, research synthesis methods have blossomed [@shadish2015meta; @nakagawa2019research].-->
In step with this interest in replication research,
research synthesis methods have blossomed [REF].
These methods aggregate the findings of multiple studies,
and thus enable drawing overarching conclusions across multiple studies.
The present paper focuses on quantitative methods for research synthesis [e.g., @vanlissaSelectRelevantModerators2021], although qualitative [e.g., @vanlissaMappingPhenomenaRelevant2021] research synthesis methods also exist. 

A key challenge in quantitative research synthesis is dealing with between-studies heterogeneity [@higginsReevaluationRandomeffectsMetaanalysis2009].
Heterogeneity appears when, for example, studies examine the same research question in different laboratories, use idiosyncratic methods, and sample from distinct populations.
The most common quantitative research synthesis method is meta-analysis,
in which results of different studies are aggregated to estimate an aggregate effect size [@borensteinIntroductionMetaAnalysis2009].
In meta-analysis, heterogeneity can be accounted for in four ways [see @vanlissaSmallSampleMetaanalyses2020].
First, if studies are exact replications, one may assume that no heterogeneity in the outcome exists and a fixed-effect meta-analysis can be conducted to estimate the common population effect. 
Second, when heterogeneity between studies can be assumed to be because of random measurement error, random-effects meta-analysis can be used to estimate the mean of a distribution of population effects. 
Third, when there are a few systematic differences between studies, these can be accounted for using meta-regression.
Finally, when there are many potential variables that cause systematic differences and it is not known beforehand which are relevant, exploratory techniques like random forest meta-analysis and penalized meta-regression can be used to identify relevant moderators. 
Each of these approaches require making different assumptions about the nature of heterogeneity [see "Models for meta-analysis" in @vanlissaSmallSampleMetaanalyses2020].

An alternative approach for evidence synthesis that does not impose assumptions about heterogeneity is Bayesian evidence synthesis (BES) [@kuiperCombiningStatisticalEvidence2013]. 
Instead of aggregating effect sizes, BES aggregates the evidence in favor of an informative hypothesis $H_i$ across studies.
The amount of evidence for this hypothesis is expressed as a Bayes factor, BF.
A Bayes factor can be interpreted as the ratio of evidence in favor of one hypothesis $H_1$ relative to another hypothesis $H_2$, so $BF_{12} = \frac{H_1}{H_2}$.
For the purpose of this paper, we will assume that all Bayes factors are the Bayes factors of an informative hypothesis $H_i$ against its complement $H_{!i}$, $BF_{c} = \frac{H_i}{H_!i}$. 
In this context, the subscript $_!$ represents the negation operator; in other words, $H_{!i}$ means "not $H_i$".
Consequently, $BF_c$ represents the ratio of evidence in favor of $H_i$ divided by evidence against it.
Thus, BF = 10 means that the data provide ten times more support in favor of the hypothesis than against it.
When multiple studies each provide evidence for $H_i$ in the form of complement Bayes factors,
these Bayes factors can be synthesized across studies by taking their product.
The resulting product Bayes factor (PBF) summarizes the total evidence for the hypothesis.
Note that other approaches to BES exist [see "Bayesian Evidence Synthesis" in @heckReviewApplicationsBayes2022].
The only assumption of BES is that all study-specific hypotheses provide evidence about the same underlying theoretical relationship.

Although meta-analysis and BES are both research synthesis methods, they answer different research questions.
Meta-analysis estimates the value or distribution of a population effect size. 
It pools estimates of this effect size across multiple studies to obtain an overall estimate of the effect size.
It thus answers the question: Given certain assumptions about between-studies heterogeneity, what is the average population effect size?
BES, on the other hand, aggregates evidence in favor of an informative hypothesis across multiple studies.
It thus answers the question: Do all these studies support the informative hypothesis?
Both methods thus provide complementary information.

This tutorial paper introduces the first implementation of BES in useruse-friendly open source software.
A function `pbf()` was contributed to the `bain` R-package for Bayesian informative hypothesis evaluation, version `r as.character(packageVersion("bain"))`.
This paper presents a simulation study to validate the method and benchmark it against alternative evidence synthesis methods,
and illustrates several use cases through reproducible examples.

# Simulation study

<!-- Rebecca: -->
<!-- ms meer over het model vertellen. -->
<!-- Is het voor de lezer duidelijk waar ineens deze groepen vandaan komen? Zijn dat je aantal studies die je gaat samennemen? -->
<!-- Dus bijv: -->
<!-- In de populatie zijn er twee variabelen, die cooreleren met de waarde rho. We hebben k studies die elk een sample van n hebben getrokken/verzameld. Dan vertelen wat de verschillende algrotithms doen met de verschillende samples (meeste als niet alle: In elke sample wordt de sample correlatie bepaald; dan estimates samenemen en null hypo test of in each sample bewijs voor hypo en dat samennemen). Dan iets over de verschillende waardes die je bekijkt en dat elke combi een simulatie conditie is. En dan iets als: Dit doen we 1000 keer per set van condities.  -->
<!-- En dan nog iets over reliability ook. -->
The present simulation study set out to validate the PBF algorithm.
For each iteration of the simulation, we simulated a correlation coefficient in multiple samples.
<!-- CJ: Hier staat multiple samples; elders multiple groups / studies / etc. Kan je die terminologie gelijktrekken? -->
The informative hypothesis was kept constant at $H_i: \rho > .1$.
All simulation conditions were evaluated once in the presence of a true population effect,
defined as $\rho = .2$,
and once in the presence of a null effect, defined as $\rho = .1$.
A PBF > 3 was used as a decision criterion to conclude that $H_i$ was supported over its complement.
This threshold is subjective but rooted in conventional standards for interpreting Bayes factors [@jeffreys1998theory].
<!-- Rebecca: -->
<!-- waar is > 3 op gebaseerd? -->
<!-- Kass&Raftery? Als ja, bedenk wel dat dat om equaility restricties gaat (bij mijn weten). Je kan het natuurlijk gebruiken, maar plaats dan wel de kanttekening, maar is een uitgangspunt natuurlijk.  -->
<!-- Of vertel over betting odds... -->
<!-- Maar het blijft een dichitome beslissing zo.... -->
<!-- Laat je het gemiddelde PBF met range ofzo zien.... zo niet, dan zou dat toevoegen. Dan zie je meer dan een dichitome beslissing. -->
<!-- En ms kan je ook ene keer 1 en 2 gebruiken, gewoon om te zien wat er gebeurt? -->
<!-- Rebecca: -->
<!-- Now, you will say that selecting > .1 with rho .1 is false, but it is not false. It is on the border, so Hi is true.  -->
<!-- In practice it may be true half of the time, because of sampling, but still. -->
<!-- This should be reflected by BF is approx 1. -->
<!-- In your simulation, you use PBF > 3, so this may be hard to reach when the truth is on the border, but that is due to the subjective choice of 3... -->
<!-- So, I already thought that using a smaller population rho value would be of added value, but now I believe you should really include this. -->
<!-- I do see that this would end up in multiple results (once using .1 and 0.2, once (say) 0 or 0.05 and .2; but also when using other cut-off values for PBF). -->
To examine performance in a range of realistic scenarios,
several design factors were manipulated:
The number of observations per sample $n \in (50, 200, 500, 800)$,
which (assuming a known effect of $\rho = .1$) provides statistical power to reject a false null hypothesis at $\alpha = .05$ of
$(`r paste0(formatC(pwr.r.test(c(50, 200, 500, 800), .1)$power, digits = 2, format=  "f"), collapse = ", ")`)$, respectively.
the number of samples $k \in (2, 3, 10)$, where 2 is the minimum number of samples possible and 10 is a relatively large number, considering that replications are not very common.
The reliability of the two correlated variables, $\alpha \in (0.6, 0.8, 1.0)$, where 0.6 is the lowest reliability conventionally considered to be acceptable, and 1 represents perfect reliability, as is
assumed when analyzing correlations between observed items or scale scores.
For all unique combinations of the design factors, 1000 data sets were generated.

## Algorithms

The main algorithm of interest was the PBF.
As a benchmark for comparison, we included several other algorithms that might feasibly be used by researchers who intend to examine whether a hypothesis is true across several independent samples.
The first was vote counting: counting the number of significant effects.
Although this method is in use for aggregating conceptual replications,
it should not be considered good practice.
Three disadvantages are that vote counting disregards sample size,
reduces statistical power,
and does not quantify the strength of the evidence [(Scheerens, 2015):].
Our vote counting algorithm used one-sided z-tests to examine whether a null hypothesis corresponding to the informative hypothesis was rejected in the majority of samples, i.e.: $H_0: \rho = .1$, and $H_a: \rho > .1$, which corresponds to $H_i$.
Thus, for example, if $H_a$ was supported in three out of five samples, our vote counting algorithm would find overall support for $H_a$ (and, by extension, $H_i$).
The second was a random-effects meta-analysis (RMA), which is the standard in the field.
For this algorithm, the null-hypothesis was rejected if a 90% confidence interval for the overall effect size excluded $H_0$.
The third algorithm was individual participant data (IPD) meta-analysis.
Like classic meta-analysis, IPD is a multilevel model, clustered by sample.
In contrast to classic meta-analysis, IPD freely estimates variance at the first level, because raw data are available.
The PBF can be estimated using either sufficient statistics (as in meta-analysis) or using raw data (as in IPD).
It is thus informative to compare the PBF to both these methods.
IPD was also evaluated using a 90% confidence interval for the overall effect size.
<!-- Rebecca: -->
<!-- zou je dit vooraf moeten zeggen? -->
<!-- Dan zeggen dat ILP dit ook kan en dan zeggen dus dat ook als vergelijking meenemen? -->
<!-- Ms zelfs in intro dit al noemoen, dan zeggen dat er twee meta-an equivalenten zijn, waarin je null hypo testing kan doen. En we dat gaan meenemen om te vergelijken. -->


## Performance indicators

For each algorithm, inferential decisions made using the criteria described above were compared to the population status of the hypothesis (true or false).
This results in a so-called "confusion matrix",
which gives the number of decisions that were true positives (TP), true negatives (TN), false positives (FP) and false negatives (FN).
These quantities were summarized as sensitivity, ${TP}{TP + FN}$, the probability of making the inferential decision that the informative hypothesis is supported, given that it was indeed true in the population, <!-- i think sensitivity is the ability to detect an effect, given that the effect is truly there. I found that its formula is usually presented as TP/(TP+FN)-->
<!--CJ: This is all the same, see https://en.wikipedia.org/wiki/Sensitivity_and_specificity -->
and specificity, $\frac{TN}{TN+FP}$, the probability of concluding the informative hypothesis is not supported, given that it was indeed false in the population. <!-- and specificity is the ability to not acknowledge an effect, given that there indeed is none-->
The overall performance was captured by the accuracy, which represents the total proportion of correct (true positive and true negative) decisions, $\frac{TP + TN}{TP+TN+FP+FN}$.

```{r, echo = F}
library(data.table) # load in libraries
library(DT)
library(ggplot2)
# rename <- c("prodbf" = "PBF", "gpbf" = "gPBF", "tbf" = "TBF", "ipd" = "IPD", "rma" = "RMA", "allsig" = "VC")

```

# Results

```{r tabconf, echo = F, message=F}
tab <- read.csv("confusion.csv", stringsAsFactors = FALSE)
names(tab)[1] <- "Metric"
tab$Metric <- gsub("^.+_", "", tab$Metric)
tab <- tab[!tab$Metric == "lr", !grepl("iu", names(tab))]
names(tab) <- gsub("_ic", "", names(tab))
# names(tab)[match(names(rename), names(tab))] <- rename
tab <- tab[tab$Metric %in% c("sensitivity", "specificity", "accuracy"),]
rownames(tab) <- NULL
knitr::kable(tab, digits = 2, caption = "Marginal confusion matrix metrics.")
```
First, we examined overall model performance across conditions.
Recall that sensitivity is the probability of concluding that the informative hypothesis is supported, given that it was indeed true in the population,
and specificity, is the probability of concluding that it is not supported, given that it was false in the population.
The results indicate that all algorithms except PBF had low sensitivity to detect a true effect.
In contrast, specificity was very high for all algorithms except PBF.
This suggests that the other algorithms classified most conditions as negatives (no effect found),
regardless of the existence of a population effect.
The PBF trades a loss of specificity for increased sensitivity.
At the same time, the average specificity and sensitivity were approximately equal, see Table \@ref(tab:tabconf).

## Effect of simulation conditions

We examined the effect of simulation conditions on overall accuracy.
```{r tabeffect, echo = FALSE}
tab <- read.csv("effect_of_conditions.csv", stringsAsFactors = FALSE)
tab <- tab[, !grepl("(gpbf|tbf)", names(tab))]
tab <- tab[, grepl("^\\w+$", names(tab)) | (grepl("\\.{2}", names(tab)) & grepl("PBF", names(tab)))]
names(tab)[grepl("\\.{2}", names(tab))] <- paste0("vs ", gsub("(\\.|PBF|vs)", "", names(tab)[grepl("\\.{2}", names(tab))]))
knitr::kable(tab, digits = 2, caption = "Partial eta squared of the effect of each design factor on accuracy for each algorithm and for the difference between PBF and all other algorithms (e.g., vs RMA).")
```
PBF performance was most impacted by sample size $n$, followed by the number of groups $k$, and reliability.
<!-- Rebecca: Nu niet compleet? -->
<!-- Gek om steeds ene paragraaf van 1 zin te hebben iig. -->
<!-- Verder mist er denk ik info. Ik zou graag eerst meer uitleg zien. Hoe heb je dat examine, wat ga je laten zien (niet het resultaat an sich, snap je wat ik bedoel...). -->
<!-- Verder zou ik denk ik niet hier al weggeven wat je hierna gaat laten zien (en anders dat explicieteren dat dat is wat je doet). -->


### Effect of sample size

```{r fign, fig.cap="Mean performance by sample size"}
res <- readRDS("confusion_by_cond.RData")

df_plot <- melt(res, measure.vars = c("sensitivity", "specificity"),
             variable.name = "Outcome", value.name = "val")
df_plot <- as.data.frame(df_plot)
df_plot <- aggregate(df_plot$val, df_plot[c("n", "Outcome", "alg")], FUN=mean)
names(df_plot)[ncol(df_plot)] <- "mean"
# df_plot <- df_plot[ ,list(mean=mean(val)), by=c("n", "Outcome", "alg")]
ggplot(df_plot, aes(x = n, y = mean, group = interaction(Outcome, alg), shape = alg, linetype = Outcome)) +
  geom_point() +
  geom_line() +
  theme_bw() +
  labs(y = "Mean performance", x = "Sample size")
```

Figure \@ref(fig:fign) indicates that for PBF, both sensitivity and specificity increase with sample size.
The other algorithms also show increasing sensitivity, but not specificity, which is at a ceiling.
This difference explains the effect of reliability on the difference between algorithms (see Table \@ref(tab:tabeffect)).

### Effect of number of groups

```{r, fig.cap="Mean performance by number of groups"}
df_plot <- data.table::melt(res, measure.vars = c("sensitivity", "specificity"),
             variable.name = "Outcome", value.name = "val")
# df_plot <- df_plot[ ,list(mean=mean(val)), by=c("k", "Outcome", "alg")]
df_plot <- as.data.frame(df_plot)
df_plot <- aggregate(df_plot$val, df_plot[c("k", "Outcome", "alg")], FUN=mean)
names(df_plot)[ncol(df_plot)] <- "mean"
ggplot(df_plot, aes(x = k, y = mean, group = interaction(Outcome, alg), shape = alg, linetype = Outcome)) +
  geom_point() +
  geom_line() +
  theme_bw() +
  labs(y = "Mean performance", x = "Number of groups")
```
The figure indicates that, for PBF at higher levels of k, lower sensitivity is exchanged for greater specificity.
The other algorithms do not show this pattern, as their specificity is at a ceiling.
This difference in pattern of effects explains why number of groups has a moderate effect on the difference between algorithms (see Table \@ref(tab:tabeffect)).
Only VC shows decreasing sensitivity with an increasing number of groups;
this is because the probability of obtaining any false negatives increases with the number of groups.


### Effect of reliability

<!--EB: I think reliability plot is a bit confusing (The one that is saved under ./manuscript_files/figure-latex/unnamed-chunk-3-1.png). That is because the x-axis is labeled 'reliability', but the values are the values of the 'errorsd' condition. This makes that the reliability decreases, rather than increases as you move away from the origin on the x-axis -->
```{r, fig.cap="Mean performance by reliability"}
df_plot <- melt(res, measure.vars = c("sensitivity", "specificity"),
             variable.name = "Outcome", value.name = "val")
# df_plot <- df_plot[ ,list(mean=mean(val)), by=c("reliability", "Outcome", "alg")]
df_plot <- as.data.frame(df_plot)
df_plot <- aggregate(df_plot$val, df_plot[c("reliability", "Outcome", "alg")], FUN=mean)
names(df_plot)[ncol(df_plot)] <- "mean"
ggplot(df_plot, aes(x = reliability, y = mean, group = interaction(Outcome, alg), shape = alg, linetype = Outcome)) +
  geom_point() +
  geom_line() +
  theme_bw() +
  labs(y = "Mean performance", x = "Reliability")
```

The figure indicates that for PBF, at higher reliability, lower sensitivity is exchanged for greater specificity.
The other algorithms do not show this pattern, as their specificity is at a ceiling.
Their sensitivity decreases with lower reliability,
and therefore, so does their overall performance.
This difference in pattern of effects explains why reliability has a moderate effect on the difference between algorithms (see Table \@ref(tab:tabeffect)).
Note that, in contrast to other algorithms, the performance of PBF is less susceptible to reliability.


# Discussion

This simulation study validated the product Bayes factor as an alternative or complementary method to synthesize evidence across studies.
The PBF was benchmarked against other common methods for evidence synthesis.
This was done by calculating a correlation coefficient and determining the accuracy, sensitivity and specificity as metrics of performance for the methods in different plausible varying conditions.
During the simulation, the hypotheses that were tested remained constant $H_i: \rho > .1$, $H_0: \rho = .1$.
To be able to calculate the performance metrics, half of the simulated conditions contained an effect, $\rho = .2$, while the other half did not, $\rho = .1$.
<!-- EB: Why did we not use H0:r=0 and Hi:r>0 as constant hypotheses? It seems more intuitive -->
Sensitivity and specificity can be interpreted as Type I error rate and statistical power respectively and are used interchangeably.
 <!-- Because Type 1 error rate and power seem to be key terms in the discussion, we may be able to include it in the introduction: 'How does the PBF compare to other state-of-the-art algorithms with regards to Type 1 error rate, power and overall accuracy?' -->
The effect of sample size per study, number of studies to synthesize evidence over and reliability of measurement on the performance of the algorithms was investigated.

The overall performance of the PBF increased most with increasing sample size.
 <!-- EB: also, shall we rename 'performance' to 'accuracy', I know its not the same, but performance seems a bit too subjective. After all, we do not have a clear cut decision when sensitivity and specificity is balanced 'correctly' --> 
PBF's sensitivity was .57 when a sample size of 20 per sample was simulated, while for all other algorithms this was below .11.
It should be noted that in the condition where sensitivity could be calculated, the true effect $\rho$ was .2 and only .1 points above the boundary value for $H_i > .1$ and $H_0 = .1$.
It is no surprise that at smaller samples the small difference in effect size between null/informative hypothesis and true value of .1 is unlikely to be detected.
Still, even at a relatively large sample size of 500 per sample, the sensitivity of the other algorithms only rose slightly above .60, 
while for PBF this was .90.
This provides evidence that state-of-the-art evidence synthesis methods have little power to detect a true, but small effect,
even when the sample size per sample was substantial.
PBF on the other hand presents itself as a small sample size solution for evidence synthesis methods to detect true effects,
even if the effect itself can be considered small.
This is also reflected in the accuracy of the algorithms at small sample sizes, which, at a sample size of 20, was around .50 for RMA, IPD and VC.
This can be attributed to their low sensitivity and their specificity that is at ceiling level. 
By consistently not finding an effect, the algorithms are correct 50\% of the time.
At all sample sizes, the PBF was the most accurate algorithm,
although it should be noted that even at a sample size of 500, PBFs specificity was .12 percentage points too high for conventional acceptable type I error rates of .05.

With an increasing number of groups, slightly decreasing specificity was traded off for increasing sensitivity.<!--EB: I would argue that the decrease is negligible, 0.767 at k = 2, 0.763 at k = 10 -->
With an increasing number of groups, specificity remained around .76-.77 for PBF, while sensitivity increased to .87.
Again, the specificity remained at ceiling level for the other algorithms.
The sensitivity for RMA and IPD increased, but stayed below .51.
In a similar way, this supports the use of the PBF as a small sample size solution.
In this case when there are little available studies to synthesize evidence over.
It could be argued that when there are only 2 or 3 studies available to synthesize evidence over, that if a trade-off need be made, specificity is preferred over sensitivity.
It seems unwise to more easily acknowledge an effect if there is little data available.
However, the quality of the included studies should be considered before synthesizing evidence in any way.
Moreover, when the researcher performs some form of evidence synthesis, then the ability of the method to find an effect is a necessary and fair component, even if little studies are available.
<!--EB: for clarification: I mean to say that research should not be done if there is effectively no power to detect an effect anyway, because the outcome is in such a case practically pre-determined. -->
Also, even with as much as ten available studies, PBF was the only algorithm that had a conventionally acceptable power.

With increasing reliability, increasing specificity was traded off for decreasing sensitivity for PBF. <!--EB: I think its the other way around; with DECREASING reliability, specificity increases and sensitivity decreases, see my comment at the section: 'Effect of reliability' -->
With increasing reliability, decreasing specificity was traded off for increasing sensitivity for the PBF.
Also for the other algorithms, the sensitivity increased as the reliability increased.
It implies that the PBF gets more conservative whenever reliability decreases.
This can be considered beneficial as the PBF is more hesitant to acknowledge an effect as the measurements become less stable.
Even at the smallest acceptable reliability, $\alpha = .6$, PBFs sensitivity was .57, where it was below .10 for the other algorithms.
However, the specificity of the PBF when there is perfect reliability $\alpha=1$ was .63, were it was almost 1 for the other algorithms.

Overall, there was only a single marginal condition where the accuracy of PBF was lower than that of any other algorithm.
PBF scored 1-2 percentage points lower accuracy than IPD and RMA when there was perfect reliability.
Looking at the specificity and sensitivity however, this is mostly explained by the specificity at ceiling level of RMA and IPD.
Thus,
Within the scope of this simulation, PBF had relatively better inferential properties <!-- EB: higher accuracy --> than the other algorithms under consideration.
Although other algorithms had superior specificity, PBF had the highest levels of sensitivity.
Thus, PBF balanced the ability to accept an informative hypothesis in the presence of a true population effect with the ability to reject the hypothesis when there was no effect.

An important caveat is that none of the algorithms met conventional criteria for power (80%) and type I error (5%) [@cohen2013statistical].
Overall, PBF had the best performance, with a power of 76% and type I error of 24%.
Thus, if researchers intend to synthesize evidence for an informative hypothesis across heterogeneous studies, PBF may be the most suitable method - but its limitations <!-- higher probability for type 1 errors --> should be acknowledged in applied research.

Thus, the PBF can be used as a small sample size solution,
although its increased type I error rate must be taken into account.
It implies that PBF works best as a complementary method for evidence synthesis, rather than a replacement.


<!-- Conventional threshold for type 1 error: 5% -->
<!-- Conventional threshold for power: 80% -->
<!-- J. Cohen, Statistical Power Analysis for the -->
<!--  Behavioral Sciences, 2nd ed. (Erlbaum, Hillsdale, -->
<!--  NJ, 1988). This is the source of the system of power -->
<!--  analysis described here; the power values and sam -->
<!--  ple sizes of the illustrations derive from this book's -->
<!--  tables. -->
<!--  2. J -->


<!-- Rebecca: -->
<!-- Ws niet een punt voor hier, maar wel in de discussie ofzo: -->
<!-- It renders support for the hypothesis being the best in all studies. It, thus, does not render the support for the hypothesis being on average the best. -->

<!-- EB: I assume Rebecca means that we should note that 'the hypothesis being on average the best' is the hypothesis that reflects the truth. While 'the hypothesis that is best in all studies' is the hypothesis that is closest to the truth compared to the other hypotheses that were tested. It does not say anything necessarily about whether the tested hypotheses in general are good reflections of the truth. E.g, if we only test whether the earth is either flat-shaped or triangular and not acknowledge other options, then we would incorrectly conclude that the earth is flat.-->
The interpretation of the PBF should also be considered.
The PBF renders support for some hypothesis being the best in all studies, but only compared to other hypotheses it has been tested against.
This means that the tested hypothesis with most support is not necessarily the hypothesis that reflects the truth most closely.
This is because there are many other plausible hypotheses that have not been tested.
For example, testing whether the earth is flat or triangular excludes the possibility of a spherical earth.
Still, because a Bayes factor is a continuous, real numbered quantity, it is more informative than the conclusion from a null-hypothesis significance test that only has the ability to dichotomously evaluate the null-hypothesis.
The main benefit is the diversity of hypotheses that can be tested using the PBF.


<!--Rebecca: Hier ook dan artikel van BEM noemen, maar ook zeggen dat het daar om null hypotheses gaat. -->

<!--EB: dit refereert naar Rouder, J. N., & Morey, R. D. (2011). A Bayes factor meta-analysis of Bem’s ESP claim. Psychonomic Bulletin & Review, 18(4), 682-689.

The argument from Rouder and Morey (2011) is that we cannot take the PBF as a valid measure for meta-analysis. They show that calculating the BF in multiple samples and taking their product results in a different PBF than when all data is pooled and the BF is calculated after pooling. They also show another way of calculating a meta-analytic BF and that this pooled BF comes closer to that meta-analytic BF. However, this calculation assumes a constant population effect and the use of the same type of statistical model across studies. Also, pooling the data assumes that all data is drawn from a common distribution. The idiosyncratic differences between studies makes that pooling data is not a straightforward endeavour, because of the heterogeneity of the effect. Rouder and Morey do acknowledge their choice for a constant effect to be debatable.

Quote from the Rouder and Morey (2011) paper:
"It is reasonable to wonder whether the constant effect
size model underlying the meta-analytic Bayes factor is
warranted. We chose this approach because it is tractable
when researchers have access to the test statistics, rather
than the raw data."

Interesting statement. Intuitively I do not see how having only the test statistics compared to raw data makes the assumption of a constant effect more probable. After all, the test statistic is derived from the raw data...

Rebecca noticed that Rouder and Morey specifically talk about PBF not being a good measurement in the context of testing H0, which is something that we are not doing. However, the only relevant difference I see in their vs our method is that their H0 contains a single equality constraint, while we used inequality constraints. Rouder and Morey do not explicitly point this difference out and I don't see how the PBF would be a bad or good idea depending on type of constraints. -->

One possible issue with the PBF as defined in this research comes from an article of @rouder2011bayes. 
The article states that a meta-analytic Bayes factor is not as straightforward as taking the product of $k$ individual Bayes factors.
<!--EB: Their reason seems to be that Bayes factors take sample size into account, effectively reducing the ability of a Bayes factor to recognize an effect in small sample. I dont see how this invalidates the PBF. If there is an effect, then in 5 small studies, it may be true that a BF is calculated that barely support the hypothesis of an effect. However, taking the product of 5 BFs that are all 1.25 will result in a PBF of about 3.-->
Rather, they provide a way of calculating more valid a meta-analytic Bayes factor.
However, only under the assumption that the population effect is constant and the test statistic is from the same type of statistical model.
Using this form of the PBF, they show that pooling the data and calculating a single Bayes factor on the pooled data comes closer to the meta-analytic Bayes factor than taking the product of each individual study.
However, as stated earlier, one strength of the PBF lies in its ability to synthesize evidence for a hypothesis, rather than an estimate.
This means that there is no assumption of a constant population effect and/or equal types of test statistics across studies.
Furthermore, as the article of @rouder2011bayes states itself, pooling data is preferred if all data is independent and drawn from a common distribution. 
The study idiosyncrasies encountered in evidence synthesis should wary the researcher in blindly making the assumption of a common distribution.

<!-- Rebecca: -->
<!-- ook iets zeggen over vergelijkbaarheid? -->
<!-- Ws hierboven al iets over zeggen: -->
<!-- In klassieke methoden heb je equality restrictie. -->
<!-- Als rho = .1 dan null waar en moet je hem niet verwerpen, als .2 dan H0 niet waar en moet je H0 wel verwerpen. -->
<!-- Bij BF heb je Hi en als .1 dan eigenlijk zelfs Hi waar, maar op border/boundary, dus ws dan BF = ong 1. Als .2 dan Hi waar en moet je BF > 1 vinden (evt dus > 3). -->


<!--EB: What I think Rebecca means here is the following: 
In classical NHST, there is an equality restriction. we either do (r = .2) or do not (r = .1) reject H0. 
Rejecting H0 means that the data suggests that rho != .1, but rather something else than .1 (so .2, .9, -.9 etc.)
With RMA and IPD we effectively say, rho = .1 or rho != .1 which is true or not (even if the test statistic = -.8,than .1 is not in the 90% CI and we still reject  H0, even though the found effect is in the opposite direction of what we hypothesized.)

with PBF we use inequality restrictions, because an equality restriction is too specific and is almost always an inferior hypothesis.
With PBF we check if rho > .1 or rho <= .1, this means that if rho = -.8, we accept Hc and not Hi.
It is thus hard to compare the RMA/IPD and PBF decisions to reject or accept H0.

This would imply that there should be more cases where the frequentist methods acknowledge an effect. However, we see a very low sensitivity due to the small effect, so its not really a concern in this paper-->
A final remark is that of performance comparison between PBF and the other algorithms which has to do with the differences in hypothesis specification. 
For the frequentist and PBF algorithms we specified $H_0: \rho = .1$ and $H_c: \rho \leq .1$ respectively.
This means, hypothetically, that if a strong effect in the opposite direction was found, let's say $\rho = -.6$, the frequentist algorithms would reject $H_0$ while the PBF would accept $H_c$.
This implies that there should have been a higher power to detect an effect in the frequentist framework, as negative effects would reject $H_0$.
This is contrary to what was observed in this research, however, as the frequentist approaches had a low sensitivity.
Again, this has to do with the small simulated difference between the true effect and the boundary value.



# Tutorial

This tutorial about Bayesian Evidence Synthesis (BES) requires R-package `bain` version `r packageVersion('bain')` or later.
This tutorial focusses on synthesizing evidence for an informative hypotheses using the Product Bayes Factor (pbf). 
The data used to illustrate the method have been simulated based on the data presented in [@vanleeuwen2022morality].
These simulated data are available in the `bain` package. 
A more detailed description of the dataset is either provided in [@vanleeuwen2022morality] or by running `?bain::synthetic_us`, `?bain::synthetic_dk` or `?bain::synthetic_nl`.
The research question is whether there is an association between political orientation and moral dispositions.
Suitable data were collected in three countries: the United states of America, Denmark and the Netherlands.
Each sample contains multiple measures of political orientation and moral dispositions.
In the original publication, the PBF was used to aggregate evidence across scales and countries to obtain an overall measure of support for the central hypothesis.
This tutorial follows the same rationale, but uses only one effect size per sample,
and varies the way this effect size is computed to illustrate different use cases. 



```{r}
NL <- bain::synthetic_nl
DK <- bain::synthetic_dk
US <- bain::synthetic_us
```

<!-- skeleton
1.) explain calculation of single BF and what it means, refer to previous tutorial on the bayes factor
2.) introduce the case where there are multiple studies and we want to evaluate a single hypothesis
3.) introduce pbf(), its uses, outputs and interpretation
-->

### How to use bain

<!-- CJ: Is dit niet teveel herhaling van wat we in de inleiding zeggen?? -->
<!-- In conventional Null-hypothesis significance testing (NHST), as the name implies, data is analysed to generate evidence for or against the null-hypothesis.  -->
<!-- This method evaluates a single hypothesis, the null-hypothesis, and its output is dichotomous in nature: whether a theoretical relationship is probable or improbable to exist given the data.  -->
<!-- However, there are many more hypotheses that can theoretically be tested and researchers may not be interested in the null-hypothesis. They may also not only be interested in whether the hypothesis is probable, but also by how much.  -->
<!-- In the Bayesian framework, the Bayes Factor (BF) is a test statistic that gives the amount of support for any possible desired hypothesis.  -->
<!-- Because the focus of this tutorial is on BES,  -->
First, we briefly introduce the basic use of `bain::bain()`, and how to interpret its output. 
Suppose we are interested in whether the self-reported importance of family morality positively predicts a conservative socio-political orientation.
Because both scales consist of multiple items, we use structural equation modeling (SEM) to perform latent variable regression.
Our hypothesis is $H_i: \beta > 0$, where $\beta$ (beta) is the standardized regression coefficient.
Note that, for the sake of simplicity, this hypothesis corresponds to a traditional one-sided null-hypothesis test.
The original publication instead used a minimal effect size of interest, and specified $H_i: \beta > .1$.
The code below illustrates the analysis for a single sample.
For a more in-depth tutorial on `bain::bain()`, see @hoijtink2019tutorial, and for further guidance on the use of `bain::bain()` in SEM, see @REF VAN LISSA TUTORIAL.

<!--EB: For simplicities sake, I for now create simple average scores without taking into account reverse coding. This should likely be improved with factor scores -->
<!-- CJ: Keep the code SUPER simple. No functions etc -->

```{r echo = TRUE, eval = FALSE}
# Load relevant packages
library(bain)
library(lavaan)

# Specify a SEM-model with two latent variables and latent regression
model_nl <- "
family =~ fam_1 + fam_2 + fam_3
conservative =~ sepa_soc_1 + sepa_soc_2 + sepa_soc_3 + sepa_soc_4 + sepa_soc_5 +
                sepa_eco_1 + sepa_eco_2 + sepa_eco_3 + sepa_eco_4 + sepa_eco_5
conservative ~ beta * family"

# Estimate the model in lavaan
results_nl <- sem(model = model_nl, data = NL)

# Test the hypothesis that the effect size labeled 'beta' is positive
# The argument standardize = TRUE uses the standardized model parameters
bain(results_nl, hypothesis = "beta > 0", standardize = TRUE)
```
```{r echo = FALSE, eval = TRUE}
# Load relevant packages
library(bain)
library(lavaan)

# Specify a SEM-model with two latent variables and latent regression
model_nl <- "
family =~ fam_1 + fam_2 + fam_3
conservative =~ sepa_soc_1 + sepa_soc_2 + sepa_soc_3 + sepa_soc_4 + sepa_soc_5 +
                sepa_eco_1 + sepa_eco_2 + sepa_eco_3 + sepa_eco_4 + sepa_eco_5
conservative ~ beta * family"

# Estimate the model in lavaan
results_nl <- sem(model = model_nl, data = NL)

# Test the hypothesis that the effect size labeled 'beta' is positive
# The argument standardize = TRUE uses the standardized model parameters
bf_nl <- bain(results_nl, hypothesis = "beta > 0", standardize = TRUE)
bf_nl
```


<!-- Socio-political orientation and group loyalty have been aggregated to variables `soc` and `grp` respectively.  -->
<!-- Both are continuous variables with higher scores indicating either conservatism or higher group loyalty.  -->
<!-- In NHST we specify the null hypothesis that there is no association between `soc` and `grp`.  -->
<!-- One method to test the hypothesis is to calculate a standardized regression coefficient. -->

<!-- $H_0: \beta_{grp} = 0$. -->

<!-- However, we may have prior expectations that higher group loyalty is somewhat related to  higher conservatism.  -->
<!-- An informative hypothesis could be: -->

<!-- $H_i: \beta_{grp} > 0$ -->

<!-- SESAMESIM DATA: Let's say we are interested in whether the status of the site children grew up in (`site`) affects their score on a numbers test after watching Sesame Street for a year (`postnumb`). `postnumb` is a continuous variable which ranges from 0-63. `site` is a nominal variable indicating group membership with possible values 1 = disadvantaged inner city, 2 = advantaged suburban, 3 = advantaged rural, 4 = disadvantaged rural and 5 = disadvantaged Spanish speaking. In NHST we specify the null-hypothesis that the mean `postnumb` is equal for all groups in  site`: 

$H_0: \mu_{site1} =\mu_{site2} = \mu_{site3} = \mu_{site4} =\mu_{site5}$

However, we may have prior expectations that the children growing up in an advantaged site will have a higher score than children growing up in a disadvantaged site. Our informative hypothesis is:

$H_i: \mu_{site2},\mu_{site3} > \mu_{site1},\mu_{site4},\mu_{site5}$ -->

<!-- Because it is very improbable that $\beta_{grp}$ is exactly equal to 0, more information can be won by testing $H_i$. -->
<!-- We can test $H_i$ by running a linear regression on the data and passing the `lm` object with the specific hypotheses to `bain`.  -->
<!-- For a single study, such as the study that investigated the Netherlands, the Bayes factor would be obtained as follows: -->

```{r, echo = FALSE, eval = FALSE}
# load bain package
library(bain)

# specify model, allow meaningful zero-point
fit <- lm(soc ~ grp, data = NLa)

#specify Hypotheses to evaluate
hypotheses <- "grp > 0"

fit_bain <- bain(fit, hypothesis = hypotheses)
print(fit_bain)


```
<!-- SESAMESIM DATA: The output of `bain` seems to support neither hypothesis. The `fit` for both hypotheses is approximately zero which means that almost none of the data are in agreement with the restrictions the specified hypotheses pose. The `BF.c`, which is the Bayes Factor of the hypothesis versus its complement, is for both hypotheses close to zero. This implies that the complement of both hypotheses is $\frac{1}{BF.c}$ times more likely than the passed hypotheses. Finally, `PMPc`, the Posterior Model Probabilities of the specified hypotheses with $H_c$ included, is also close to zero for both specified hypothesis. This means that *out of* the specified hypotheses and their complement, the probability of $H_c$ being true is 99.9%. -->

Note that the hypothesis is strongly supported when compared to its complement, $BF_c `report(bf_nl$fit$BF.c[1])`$.

### Aggregating evidence across studies

As mentioned before, suitable data were collected to test the substantive hypothesis in three countries.
There are differences between countries that prevent analyzing these data as a multilevel model, however.
For instance, conservatism was measured using different scales.
This is an appropriate situation to use the PBF to aggregate evidence across countries.
First, we estimate a latent regression model for the remaining two countries, taking care to label the parameter of interest with the same label in all samples.
Then, we bind all three SEM-models in a list, and call PBF to evaluate the hypothesis of interest on all models and aggregate the evidence.

```{r, eval = FALSE, echo = TRUE}
# Specify the models for DK and US
model_dk <- "
family =~ fam_1 + fam_2 + fam_3
conservative =~ sepa_soc_1 + sepa_soc_2 + sepa_soc_3 + sepa_soc_4 + sepa_soc_5 +
                sepa_eco_1 + sepa_eco_2 + sepa_eco_3 + sepa_eco_4 + sepa_eco_5
conservative ~ beta * family"
model_us <- "
family =~ fam_1 + fam_2 + fam_3
conservative =~ secs_soc_1 + secs_soc_2 + secs_soc_3 + secs_soc_4 + secs_soc_5 +
                secs_soc_6 + secs_soc_7 + secs_eco_1 + secs_eco_2 + secs_eco_3 +
                secs_eco_4 + secs_eco_5
conservative ~ beta * family"

# Estimate the model in lavaan
results_dk <- sem(model = model_dk, data = DK)
results_us <- sem(model = model_us, data = US)

# Bind the models into a list
results <- list(results_nl, results_dk, results_us)
# Test the hypothesis that the effect size labeled 'beta' is positive
pbf(results, hypothesis = "beta > 0", standardize = TRUE)
```
```{r, eval = TRUE, echo = FALSE}
# Specify the models for DK and US
model_dk <- "
family =~ fam_1 + fam_2 + fam_3
conservative =~ sepa_soc_1 + sepa_soc_2 + sepa_soc_3 + sepa_soc_4 + sepa_soc_5 +
                sepa_eco_1 + sepa_eco_2 + sepa_eco_3 + sepa_eco_4 + sepa_eco_5
conservative ~ beta * family"
model_us <- "
family =~ fam_1 + fam_2 + fam_3
conservative =~ secs_soc_1 + secs_soc_2 + secs_soc_3 + secs_soc_4 + secs_soc_5 +
                secs_soc_6 + secs_soc_7 + secs_eco_1 + secs_eco_2 + secs_eco_3 +
                secs_eco_4 + secs_eco_5
conservative ~ beta * family"

# Estimate the model in lavaan
results_dk <- sem(model = model_dk, data = DK)
results_us <- sem(model = model_us, data = US)

# Bind the models into a list
results <- list(results_nl, results_dk, results_us)
# Test the hypothesis that the effect size labeled 'beta' is positive
bf_tot <- pbf(results, hypothesis = "beta > 0", standardize = TRUE)
bf_tot
```

As the BF in all three samples is positive, the resulting PBF is very large, $PBF `report(bf_tot[1,1])`$.
We can thus conclude that the central hypothesis receives overwhelming evidence across samples.
<!-- In the first use-case of `pbf()`, an `lm()` is ran for each sample. All `lm` objects are stored in a list which is passed to `pbf()`. -->
<!-- `pbf()` supports lists of all objects that `bain()` also supports. All supported types are specified in bain vignette *REF*. One exception is the use of named vectors, which will be covered later. -->
<!-- ```{r} -->
<!-- lm_sample1 <- lm(postnumb~site-1, sample1) -->
<!-- lm_sample2 <- lm(postnumb~site-1, sample2) -->
<!-- lm_sample3 <- lm(postnumb~site-1, sample3) -->

<!-- lm_allsamples <- list(lm_sample1, lm_sample2, lm_sample3) -->

<!-- pbf(lm_allsamples, hypotheses) -->
<!-- ``` -->

<!-- The function returns a `data.frame` containing the passed hypotheses in the rows and in the columns the PBF and the individual Bayes Factor for each sample individually. The `PBF` of both hypotheses is close to zero, meaning that the combined evidence for both hypotheses over all studies is low. When looking at the individual Bayes Factors, we also see that the PBF is lower than all of the Bayes Factors individually. For sample 3 the specified hypotheses are even more likely than their complement. This example illustrates another strength of the PBF; being able to evaluate a hypothesis more extremely than the most extreme individual evaluation. If all samples do not support a hypothesis, than the number of times the hypothesis is not supported should be included in its evaluation. This leads to a more extreme combined evaluation than any individual evaluation. Likewise, if there is very strong support in only a small minority of the samples, while the majority of samples shows no support at all, than it is likely that the hypothesis is not true. -->

<!-- Sometimes there is access to the data of not only a single, but multiple studies that all contain the necessary information to evaluate our hypothesis $H_i$.  -->
<!-- To evaluate $H_i$, it is possible to synthesize the support for $H_i$ over all studies using the pbf calculated with `pbf()`.  -->
<!-- In this example, data to evaluate $H_i$ is available for three countries. -->

<!-- In the first use-case of `pbf()`, an `lm()` is ran for each country.  -->
<!-- All `lm` objects are stored in a list which is passed to `pbf()`. -->
<!-- `pbf()` supports lists of all objects that `bain()` also supports.  -->
<!-- All supported types are specified in the `bain` vignette.  -->
<!-- One exception is the use of named vectors, which will be covered later. -->
<!-- ```{r} -->
<!-- fit_US <- lm(soc ~ grp + her, data = USa) -->
<!-- fit_DK <- lm(soc ~ grp + her, data = DKa) -->
<!-- fit_NL <- lm(soc ~ grp + her, data = NLa) -->

<!-- all_fits <- list(fit_US, fit_DK, fit_NL) -->

<!-- pbf(all_fits, hypotheses) -->
<!-- ``` -->

### Using bain objects

The `pbf()` function also accepts multiple `bain` objects.
This makes it possible to, for example, evaluate different sets of hypotheses on different data sets before using the resulting `bain` objects to aggregate the evidence for all common hypotheses across datasets.
The example below illustrates this use case.
As before, all analyses share one hypotheses in common ($H_i: \beta_{fam} > 0$),
but the Dutch sample now contains a sample-specific hypothesis regarding the effect of group morality, namely that $\beta_{grp} < 0$.
The `pbf()` function is called on a list of `bain` objects.
Note that, in this case, `pbf()` does not require an argument `hypothesis`, as the hypotheses are contained in the `bain` objects.

```{r, echo = TRUE, eval = FALSE}
# Add the additional predictor to the model, label the effect beta2
model_nl <- paste(model_nl, "group =~ grp_1 + grp_2 + grp_3
                             conservative ~ beta2 * group", sep = "\n")

# Estimate the model in lavaan
results_nl <- sem(model = model_nl, data = NL)

# Obtain BF for each sample; note that the Dutch sample has two hypotheses
bf_nl <- bain(results_nl, hypothesis = "beta > 0;beta2 < 0", standardize = TRUE)
bf_dk <- bain(results_dk, "beta > 0")
bf_us <- bain(results_us, "beta > 0")

# Bind bain objects into a list
bfs <- list(bf_nl, bf_dk, bf_us)

# Call pbf on that list
pbf(bfs)
```

As can be seen, the results are equivalent to the results in the previous example.
The sample-specific hypothesis has been left out, and common hypotheses are retained and aggregated.
If there are no common hypotheses across all objects, `pbf()` throws an error.

```{r, echo = FALSE, eval = FALSE}
bains_different_hyps <- list(
  bain(fit_US, "grp = 0"),
  bain(fit_DK, "grp > .1"),
  bain(fit_NL, "grp < .1")
)

tryCatch({
  pbf(bains_different_hyps)},
  error = function(e){message(e)}
      )

```
<!--The function returns an error stating that the samples have no hypothesis in common and thus the PBF cannot be calculated.-->

### Using sufficient statistics

A third use-case occurs when the raw data from different samples is not available.
This may happen, for example, when aggregating findings from the published literature (similar to meta-analysis).
In this case, one can use the default interface of `bain`, as explained in [@hoijtink2019tutorial].
This function requires four arguments: A named vector of parameter estimates, their asymptotic covariance matrix, the original sample size, and the number of within-group and between-group parameters.
Note that, when analyzing a single parameter per sample, the standard error is sufficient to construct the asymptotic covariance matrix.
Thus, this method is suitable for data that have been prepared for meta-analysis.
Importantly, unlike meta-analysis, the present method is suitable for conceptual replications.
It does not require uniform effect size measures across studies.
The example below illustrates how to aggregate evidence in favor of one hypotheses across three studies that each used different methods.
<!-- CJ: What I'd like to do here is show how to aggregate a t-test, a regression coefficient and a correlation coefficient -->

The present use case effectively tests the following hypothesis:
*There is a positive association between family morality and political conservatism*.
This conceptual hypothesis is tested differently in the three samples, resulting in three different types of statistics and distinct sample-specific hypotheses:

1. A t-test was performed using the NL data; using Cohen's D gives $H_i^{NL}: \delta_{conservative > liberal} > 0$
1. A bivariate regression coefficient for `grp` was calculated using the DK data, giving $H_i^{DK}: b_{fam} > 0$
1. A correlation coefficient was calculated using the US data, giving $H_i^{US}: \rho_{fam,con} > 0$

Note that we intentionally manipulate the data to fit these types of analyses;
for example, we compute mean scale scores and dichotomize a continuous variable to conduct a t-test.
We do not advocate these practices for applied research.

First we obtain the relevant parameter estimates and their sampling variances,
which allows us to test the specific hypotheses in `bain`:

```{r eval = TRUE, echo = TRUE}
# Create mean scale scores
NL <- data.frame(
  family = rowMeans(NL[c("fam_1", "fam_2", "fam_3")]),
  conservative = rowMeans(NL[c("sepa_soc_1", "sepa_soc_2", "sepa_soc_3",
                               "sepa_soc_4", "sepa_soc_5", "sepa_eco_1",
                               "sepa_eco_2", "sepa_eco_3", "sepa_eco_4",
                               "sepa_eco_5")]))
DK <- data.frame(
  family = rowMeans(DK[c("fam_1", "fam_2", "fam_3")]),
  conservative = rowMeans(DK[c("sepa_soc_1", "sepa_soc_2", "sepa_soc_3",
                               "sepa_soc_4", "sepa_soc_5", "sepa_eco_1",
                               "sepa_eco_2", "sepa_eco_3", "sepa_eco_4",
                               "sepa_eco_5")]))

US <- data.frame(
  family = rowMeans(US[c("fam_1", "fam_2", "fam_3")]),
  conservative = rowMeans(US[c("secs_soc_1", "secs_soc_2", "secs_soc_3",
                               "secs_soc_4", "secs_soc_5", "secs_soc_6",
                               "secs_soc_7", "secs_eco_1", "secs_eco_2",
                               "secs_eco_3", "secs_eco_4", "secs_eco_5")]))

# NL: Conduct t-test using Cohen's D
NL$group <- cut(NL$conservative, breaks = 2, labels = c("liberal", "conservative"))
sample_sizes <- table(NL$group)
sds <- tapply(NL$family, NL$group, sd)
pooled_sd <- sqrt(sum((sample_sizes - 1) * sds) / (sum(sample_sizes) - 2))
NL_est <- diff(tapply(NL$family, NL$group, mean)) / pooled_sd
NL_var <- (sum(sample_sizes) / prod(sample_sizes)) + (NL_est^2 / (2*sum(sample_sizes)))

# DK: Conduct bivariate regression
DK_fit <- lm(conservative ~ family, data = DK)
DK_est <- coef(DK_fit)["family"]
DK_var <- vcov(DK_fit)["family", "family"]

# US: Correlation coefficient
US_est <- cor(US)[1, 2]
US_var <- (1 - US_est^2)^2 / (nrow(US) - 1)

# Name the estimates so hypotheses will be the same
names(NL_est) <- names(DK_est) <- names(US_est) <- "parameter"
```

Then, we use `bain.default()` to evaluate the central hypothesis on each parameter estimate. 
The pbf can then be calculated as in the previous use case.

```{r}
# Use bain.default() to obtain BF for the central hypothesis 
NL_bain <- bain(x = NL_est, 
                Sigma = matrix(NL_var, 1, 1),
                n = nrow(NL),
                hypothesis = "parameter > 0",
                joint_parameters = 1)
DK_bain <- bain(x = DK_est,
                Sigma = matrix(DK_var, 1, 1),
                n = nrow(DK),
                hypothesis = "parameter > 0",
                joint_parameters = 1)
US_bain <- bain(x = US_est,
                Sigma = matrix(US_var, 1, 1),
                n = nrow(US),
                hypothesis = "parameter > 0",
                joint_parameters = 1)

# Aggregate evidence using pbf()
pbf(list(US_bain, DK_bain, NL_bain))
```

The results suggest substantial evidence in favor of the hypothesis that there is a positive association between family morality and political conservatism.
Although each study used a different method to assess this hypothesis,
their evidence can be synthesized using the pbf.

<!--
```{r, eval = FALSE, echo = FALSE}
get_estimates <- function(sample){
  number_sites <- length(levels(sesamesim$site))
  
  sample_size <- nrow(sample)
  
  site_estimates <- sapply(1:number_sites, function(i){
    rbind(
      mean(sample[sample['site'] == i, "postnumb"]),
      sd(sample[sample['site'] == i, "postnumb"])
      )
    })
  
  est_names <- sprintf("site%d", 1:number_sites)
  site_means <- site_estimates[1,]
  names(site_means) <- est_names
  
  covmat <- diag((site_estimates[2,] / sqrt(sample_size))^2)
  dimnames(covmat) <- list(est_names, est_names)
  return(list(means = site_means, covmat = covmat, sample_size = sample_size))
}

ests_sample1 <- get_estimates(sample1)
ests_sample2 <- get_estimates(sample2)
ests_sample3 <- get_estimates(sample3)

```

Then, each object is converted to a `bain` object with these estimates
```{r}
bain_sample1_ests <- bain(x = ests_sample1[['means']], 
                          Sigma = ests_sample1[['covmat']], 
                          n = ests_sample1[['sample_size']], 
                          hypothesis = hypotheses)

bain_sample2_ests <- bain(x = ests_sample2[['means']], 
                          Sigma = ests_sample2[['covmat']], 
                          n = ests_sample2[['sample_size']], 
                          hypothesis = hypotheses)

bain_sample3_ests <- bain(x = ests_sample3[['means']], 
                          Sigma = ests_sample3[['covmat']], 
                          n = ests_sample3[['sample_size']], 
                          hypothesis = hypotheses)

bain_allsamples_ests = list(bain_sample1_ests, bain_sample2_ests, bain_sample3_ests)
```

And finally, on these objects, `pbf()` can be called.
```{r}
pbf(bain_allsamples_ests)
```
-->
<!-- It is interesting that these estimates are not identical to the other examples... Is that because we do not have ALL information available in this use-case, or is something off in the script? -->
<!-- CJ: We should ask Herbert about this, but this example won't make it into this paper. If you want, you can send Herbert a reproducible example of the 'problem'!-->


\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
















