@book{borensteinIntroductionMetaAnalysis2009,
  title = {Introduction to {{Meta-Analysis}}},
  author = {Borenstein, Michael and Hedges, Larry V. and Higgins, Julian P. T. and Rothstein, Hannah R.},
  date = {2009},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/9780470743386},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/9780470743386.refs/summary},
  isbn = {978-0-470-74338-6},
  langid = {english},
  keywords = {meta-analysis,Meta-Analysis as Topic},
  file = {C\:\\Users\\vanlissa\\Zotero\\storage\\8U6XAR8B\\Borenstein - Introduction to Meta-analysis.pdf;C\:\\Users\\vanlissa\\Zotero\\storage\\R975CG6N\\summary.html}
}

@article{brembsPrestigiousScienceJournals2018,
  title = {Prestigious {{Science Journals Struggle}} to {{Reach Even Average Reliability}}},
  author = {Brembs, Björn},
  date = {2018-02-20},
  journaltitle = {Frontiers in Human Neuroscience},
  shortjournal = {Front. Hum. Neurosci.},
  volume = {12},
  publisher = {Frontiers},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2018.00037},
  url = {https://www.frontiersin.org/articles/10.3389/fnhum.2018.00037},
  urldate = {2024-04-08},
  abstract = {In which journal a scientist publishes is considered one of the most crucial factors determining their career. The underlying common assumption is that only the best scientists manage to publish in a highly selective tier of the most prestigious journals. However, data from several lines of evidence suggest that the methodological quality of scientific experiments does not increase with increasing rank of the journal. On the contrary, an accumulating body of evidence suggests the inverse: methodological quality and, consequently, reliability of published research works in several fields may be decreasing with increasing journal rank. The data supporting these conclusions circumvent confounding factors such as increased readership and scrutiny for these journals, focussing instead on quantifiable indicators of methodological soundness in the published literature, relying on, in part, semi-automated data extraction from often thousands of publications at a time. With the accumulating evidence over the last decade grew the realization that the very existence of scholarly journals, due to their inherent hierarchy, constitutes one of the major threats to publicly funded science: hiring, promoting and funding scientists who publish unreliable science eventually erodes public trust in science.},
  langid = {english},
  keywords = {Journal Ranking,journals,Reliability,Reproducibility of Results,Science Policy},
  file = {C:\Users\vanlissa\Zotero\storage\Y8S4EKFK\Brembs - 2018 - Prestigious Science Journals Struggle to Reach Eve.pdf}
}

@article{guApproximatedAdjustedFractional2018,
  title = {Approximated Adjusted Fractional {{Bayes}} Factors: {{A}} General Method for Testing Informative Hypotheses},
  shorttitle = {Approximated Adjusted Fractional {{Bayes}} Factors},
  author = {Gu, Xin and Mulder, Joris and Hoijtink, Herbert},
  date = {2018},
  journaltitle = {British Journal of Mathematical and Statistical Psychology},
  volume = {71},
  number = {2},
  pages = {229--261},
  issn = {2044-8317},
  doi = {10.1111/bmsp.12110},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/bmsp.12110},
  urldate = {2020-01-15},
  abstract = {Informative hypotheses are increasingly being used in psychological sciences because they adequately capture researchers’ theories and expectations. In the Bayesian framework, the evaluation of informative hypotheses often makes use of default Bayes factors such as the fractional Bayes factor. This paper approximates and adjusts the fractional Bayes factor such that it can be used to evaluate informative hypotheses in general statistical models. In the fractional Bayes factor a fraction parameter must be specified which controls the amount of information in the data used for specifying an implicit prior. The remaining fraction is used for testing the informative hypotheses. We discuss different choices of this parameter and present a scheme for setting it. Furthermore, a software package is described which computes the approximated adjusted fractional Bayes factor. Using this software package, psychological researchers can evaluate informative hypotheses by means of Bayes factors in an easy manner. Two empirical examples are used to illustrate the procedure.},
  langid = {english},
  keywords = {fractional Bayes factor,informative hypothesis,normal approximation,prior sensitivity},
  file = {C\:\\Users\\vanlissa\\Zotero\\storage\\Y2VH4LF8\\Gu et al_2018_Approximated adjusted fractional Bayes factors.pdf;C\:\\Users\\vanlissa\\Zotero\\storage\\K4PGTHHV\\bmsp.html}
}

@article{heckReviewApplicationsBayes2022,
  title = {A Review of Applications of the {{Bayes}} Factor in Psychological Research.},
  author = {Heck, Daniel W. and Boehm, Udo and Böing-Messing, Florian and Bürkner, Paul-Christian and Derks, Koen and Dienes, Zoltan and Fu, Qianrao and Gu, Xin and Karimova, Diana and Kiers, Henk A. L. and Klugkist, Irene and Kuiper, Rebecca M. and Lee, Michael D. and Leenders, Roger and Leplaa, Hidde J. and Linde, Maximilian and Ly, Alexander and Meijerink-Bosman, Marlyne and Moerbeek, Mirjam and Mulder, Joris and Palfi, Bence and Schönbrodt, Felix D. and Tendeiro, Jorge N. and family=Bergh, given=Don, prefix=van den, useprefix=true and Van Lissa, Caspar J. and family=Ravenzwaaij, given=Don, prefix=van, useprefix=true and Vanpaemel, Wolf and Wagenmakers, Eric-Jan and Williams, Donald R. and Zondervan-Zwijnenburg, Mariëlle and Hoijtink, Herbert},
  date = {2022-03-17},
  journaltitle = {Psychological Methods},
  shortjournal = {Psychological Methods},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000454},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/met0000454},
  urldate = {2022-05-11},
  abstract = {The last 25 years have shown a steady increase in attention for the Bayes factor as a tool for hypothesis evaluation and model selection. The present review highlights the potential of the Bayes factor in psychological research. We discuss six types of applications: Bayesian evaluation of point null, interval, and informative hypotheses, Bayesian evidence synthesis, Bayesian variable selection and model averaging, and Bayesian evaluation of cognitive models. We elaborate what each application entails, give illustrative examples, and provide an overview of key references and software with links to other applications. The paper is concluded with a discussion of the opportunities and pitfalls of Bayes factor applications and a sketch of corresponding future research lines.},
  langid = {english},
  file = {C:\Users\vanlissa\Zotero\storage\MJBNPK7K\Heck et al. - 2022 - A review of applications of the Bayes factor in ps.pdf}
}

@article{hedgesVotecountingMethodsResearch1980,
  title = {Vote-Counting Methods in Research Synthesis},
  author = {Hedges, Larry V. and Olkin, Ingram},
  date = {1980},
  journaltitle = {Psychological Bulletin},
  volume = {88},
  pages = {359--369},
  publisher = {American Psychological Association},
  location = {US},
  issn = {1939-1455},
  doi = {10.1037/0033-2909.88.2.359},
  abstract = {Reviews of research usually rely on counts of the number of times the treatment group mean exceeds the control group mean by an amount that is statistically significant. The treatment is said to have a positive effect if the proportion of such positive significant results is large. This procedure is shown to have extremely low power for the combination of treatment-effect sizes and sample sizes usually found in social science research. Surprisingly, the power of this procedure decreases as the number of studies reviewed increases. Three alternative counting procedures that permit estimation of the standardized mean difference between treatment and control groups (effect size) are described. Methods for obtaining confidence intervals for the effect size are also presented. For 1 procedure, the effect size can be estimated even when the sample consists only of studies that have statistically significant results. (7 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Central Tendency Measures,Statistical Analysis,Treatment Effectiveness Evaluation},
  file = {C:\Users\vanlissa\Zotero\storage\5LCKUAYC\1980-29312-001.html}
}

@article{higginsReevaluationRandomeffectsMetaanalysis2009,
  title = {A Re-Evaluation of Random-Effects Meta-Analysis},
  author = {Higgins, Julian P. T. and Thompson, Simon G. and Spiegelhalter, David J},
  date = {2009-01},
  journaltitle = {Journal of the Royal Statistical Society. Series A, (Statistics in Society)},
  shortjournal = {J R Stat Soc Ser A Stat Soc},
  volume = {172},
  number = {1},
  eprint = {19381330},
  eprinttype = {pmid},
  pages = {137--159},
  issn = {0964-1998},
  doi = {10.1111/j.1467-985X.2008.00552.x},
  url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2667312/},
  abstract = {Meta-analysis in the presence of unexplained heterogeneity is frequently undertaken by using a random-effects model, in which the effects underlying different studies are assumed to be drawn from a normal distribution. Here we discuss the justification and interpretation of such models, by addressing in turn the aims of estimation, prediction and hypothesis testing. A particular issue that we consider is the distinction between inference on the mean of the random-effects distribution and inference on the whole distribution. We suggest that random-effects meta-analyses as currently conducted often fail to provide the key results, and we investigate the extent to which distribution-free, classical and Bayesian approaches can provide satisfactory methods. We conclude that the Bayesian approach has the advantage of naturally allowing for full uncertainty, especially for prediction. However, it is not without problems, including computational intensity and sensitivity to a priori judgements. We propose a simple prediction interval for classical meta-analysis and offer extensions to standard practice of Bayesian meta-analysis, making use of an example of studies of ‘set shifting’ ability in people with eating disorders.},
  pmcid = {PMC2667312},
  file = {C:\Users\vanlissa\Zotero\storage\9RPWEBFF\Higgins et al. - 2009 - A re-evaluation of random-effects meta-analysis.pdf}
}

@article{hoijtinkTutorialTestingHypotheses2019,
  title = {A Tutorial on Testing Hypotheses Using the {{Bayes}} Factor},
  author = {Hoijtink, Herbert and Mulder, Joris and family=Lissa, given=Caspar, prefix=van, useprefix=true and Gu, Xin},
  date = {2019},
  journaltitle = {Psychological Methods},
  volume = {24},
  number = {5},
  pages = {539--556},
  issn = {1939-1463(Electronic),1082-989X(Print)},
  doi = {10.1037/met0000201},
  abstract = {Learning about hypothesis evaluation using the Bayes factor could enhance psychological research. In contrast to null-hypothesis significance testing it renders the evidence in favor of each of the hypotheses under consideration (it can be used to quantify support for the null-hypothesis) instead of a dichotomous reject/do-not-reject decision; it can straightforwardly be used for the evaluation of multiple hypotheses without having to bother about the proper manner to account for multiple testing; and it allows continuous reevaluation of hypotheses after additional data have been collected (Bayesian updating). This tutorial addresses researchers considering to evaluate their hypotheses by means of the Bayes factor. The focus is completely applied and each topic discussed is illustrated using Bayes factors for the evaluation of hypotheses in the context of an ANOVA model, obtained using the R package bain. Readers can execute all the analyses presented while reading this tutorial if they download bain and the R-codes used. It will be elaborated in a completely nontechnical manner: what the Bayes factor is, how it can be obtained, how Bayes factors should be interpreted, and what can be done with Bayes factors. After reading this tutorial and executing the associated code, researchers will be able to use their own data for the evaluation of hypotheses by means of the Bayes factor, not only in the context of ANOVA models, but also in the context of other statistical models. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {Analysis of Variance,Errors,Hypothesis Testing,Null Hypothesis Testing,Statistical Probability,Statistical Significance},
  file = {C\:\\Users\\vanlissa\\Zotero\\storage\\DNQD4RXS\\Hoijtink et al_2019_A tutorial on testing hypotheses using the Bayes factor.pdf;C\:\\Users\\vanlissa\\Zotero\\storage\\8UP5I6ZT\\2019-07157-001.html}
}

@article{klugkistBayesianEvidenceSynthesis2023,
  title = {Bayesian Evidence Synthesis for Informative Hypotheses: {{An}} Introduction},
  shorttitle = {Bayesian Evidence Synthesis for Informative Hypotheses},
  author = {Klugkist, Irene and Volker, Thom Benjamin},
  date = {2023-09-07},
  journaltitle = {Psychological Methods},
  shortjournal = {Psychol Methods},
  eprint = {37676166},
  eprinttype = {pmid},
  issn = {1939-1463},
  doi = {10.1037/met0000602},
  abstract = {To establish a theory one needs cleverly designed and well-executed studies with appropriate and correctly interpreted statistical analyses. Equally important, one also needs replications of such studies and a way to combine the results of several replications into an accumulated state of knowledge. An approach that provides an appropriate and powerful analysis for studies targeting prespecified theories is the use of Bayesian informative hypothesis testing. An additional advantage of the use of this Bayesian approach is that combining the results from multiple studies is straightforward. In this article, we discuss the behavior of Bayes factors in the context of evaluating informative hypotheses with multiple studies. By using simple models and (partly) analytical solutions, we introduce and evaluate Bayesian evidence synthesis (BES) and compare its results to Bayesian sequential updating. By doing so, we clarify how different replications or updating questions can be evaluated. In addition, we illustrate BES with two simulations, in which multiple studies are generated to resemble conceptual replications. The studies in these simulations are too heterogeneous to be aggregated with conventional research synthesis methods. (PsycInfo Database Record (c) 2023 APA, all rights reserved).},
  langid = {english}
}

@article{kuiperCombiningStatisticalEvidence2013,
  title = {Combining {{Statistical Evidence From Several Studies}}: {{A Method Using Bayesian Updating}} and an {{Example From Research}} on {{Trust Problems}} in {{Social}} and {{Economic Exchange}}},
  shorttitle = {Combining {{Statistical Evidence From Several Studies}}},
  author = {Kuiper, Rebecca M. and Buskens, Vincent and Raub, Werner and Hoijtink, Herbert},
  date = {2013-02},
  journaltitle = {Sociological Methods \& Research},
  shortjournal = {Sociological Methods \& Research},
  volume = {42},
  number = {1},
  pages = {60--81},
  issn = {0049-1241, 1552-8294},
  doi = {10.1177/0049124112464867},
  url = {http://journals.sagepub.com/doi/10.1177/0049124112464867},
  urldate = {2022-05-11},
  langid = {english},
  file = {C:\Users\vanlissa\Zotero\storage\D5DF4Y37\Kuiper et al. - 2013 - Combining Statistical Evidence From Several Studie.pdf}
}

@article{kweeAnxiolyticEffectsEndocannabinoid2023,
  title = {Anxiolytic Effects of Endocannabinoid Enhancing Compounds: {{A}} Systematic Review and Meta-Analysis},
  shorttitle = {Anxiolytic Effects of Endocannabinoid Enhancing Compounds},
  author = {Kwee, Caroline M. B. and Leen, Nadia A. and Van der Kamp, Rian C. and Van Lissa, Caspar J. and Cath, Danielle C. and Groenink, Lucianne and Baas, Johanna M. P.},
  date = {2023-07-01},
  journaltitle = {European Neuropsychopharmacology},
  shortjournal = {European Neuropsychopharmacology},
  volume = {72},
  pages = {79--94},
  issn = {0924-977X},
  doi = {10.1016/j.euroneuro.2023.04.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0924977X23000676},
  urldate = {2024-01-31},
  abstract = {The endocannabinoid system is a promising candidate for anxiolytic therapy, but translation to the clinic has been lagging. We meta-analyzed the evidence for anxiety-reduction by compounds that facilitate endocannabinoid signaling in humans and animals. To identify areas of specific potential, effects of moderators were assessed. Literature was searched in Pubmed and Embase up to May 2021. A placebo/vehicle-control group was required and in human studies, randomization. We excluded studies that co-administered other substances. Risk of bias was assessed with SYRCLE's RoB tool and Cochrane RoB 2.0. We conducted three-level random effects meta-analyses and explored sources of heterogeneity using Bayesian regularized meta-regression (BRMA). The systematic review yielded 134 studies. We analyzed 120 studies (114 animal, 6 human) that investigated cannabidiol (CBD, 61), URB597 (39), PF-3845 (6) and AM404 (14). Pooled effects on conditioned and unconditioned anxiety in animals (with the exception of URB597 on unconditioned anxiety) and on experimentally induced anxiety in humans favored the investigational drugs over placebo/vehicle. Publication year was negatively associated with effects of CBD on unconditioned anxiety. Compared to approach avoidance tests, tests of repetitive-compulsive behavior were associated with larger effects of CBD and URB597, and the social interaction test with smaller effects of URB597. Larger effects of CBD on unconditioned anxiety were observed when anxiety pre-existed. Studies reported few side effects at therapeutic doses. The evidence quality was low with indications of publication bias. More clinical trials are needed to translate the overall positive results to clinical applications.},
  keywords = {Anandamide,Anxiety disorders,Cannabidiol,Fatty-acid amide hydrolase,Meta-analysis,Therapeutics},
  file = {C:\Users\vanlissa\Zotero\storage\3WIMJAUP\Kwee et al. - 2023 - Anxiolytic effects of endocannabinoid enhancing co.pdf}
}

@article{lakensEquivalenceTestsPractical2017,
  title = {Equivalence {{Tests}}: {{A Practical Primer}} for t {{Tests}}, {{Correlations}}, and {{Meta-Analyses}}},
  shorttitle = {Equivalence {{Tests}}},
  author = {Lakens, Daniël},
  date = {2017-05-01},
  journaltitle = {Social Psychological and Personality Science},
  volume = {8},
  number = {4},
  pages = {355--362},
  publisher = {SAGE Publications Inc},
  issn = {1948-5506},
  doi = {10.1177/1948550617697177},
  url = {https://doi.org/10.1177/1948550617697177},
  urldate = {2024-04-01},
  abstract = {Scientists should be able to provide support for the absence of a meaningful effect. Currently, researchers often incorrectly conclude an effect is absent based a nonsignificant result. A widely recommended approach within a frequentist framework is to test for equivalence. In equivalence tests, such as the two one-sided tests (TOST) procedure discussed in this article, an upper and lower equivalence bound is specified based on the smallest effect size of interest. The TOST procedure can be used to statistically reject the presence of effects large enough to be considered worthwhile. This practical primer with accompanying spreadsheet and R package enables psychologists to easily perform equivalence tests (and power analyses) by setting equivalence bounds based on standardized effect sizes and provides recommendations to prespecify equivalence bounds. Extending your statistical tool kit with equivalence tests is an easy way to improve your statistical and theoretical inferences.},
  langid = {english},
  file = {C:\Users\vanlissa\Zotero\storage\6GCZC59E\Lakens - 2017 - Equivalence Tests A Practical Primer for t Tests,.pdf}
}

@article{lavelleWhenCrisisBecomes2021,
  title = {When a {{Crisis Becomes}} an {{Opportunity}}: {{The Role}} of {{Replications}} in {{Making Better Theories}}},
  shorttitle = {When a {{Crisis Becomes}} an {{Opportunity}}},
  author = {Lavelle, Jane Suilin},
  date = {2021-04-14},
  journaltitle = {The British Journal for the Philosophy of Science},
  shortjournal = {The British Journal for the Philosophy of Science},
  pages = {714812},
  issn = {0007-0882, 1464-3537},
  doi = {10.1086/714812},
  url = {https://www.journals.uchicago.edu/doi/10.1086/714812},
  urldate = {2022-03-01},
  abstract = {While it is widely acknowledged that psychology is in the throes of a replication ‘crisis’, relatively little attention has been paid to the role theory plays in our evaluation of replications as ‘failed’ or ‘successful’. This paper applies well-known arguments in philosophy of science about the interplay between theory and experiment to a contemporary case study of infants’ understanding of false belief (Onishi and Baillargeon [2005]), and attempts to replicate it. It argues that the lack of consensus about over-arching theories informing both the concepts under study and the methodologies used to track them means that researchers disagree over which experiments constitute replications of the original. The second part of the paper places this specific debate within a broader discussion of the replication crisis as a crisis of ‘theory’, developing work by Muthukrishna and Henrich ([2018]) and Bird ([2018]). Bird argues that the lack of agreed over-arching theories in psychology means that a high rate of replication failure is to be expected; this paper agrees with his diagnosis but challenges his proposal that more replication will resolve the problem.},
  langid = {english},
  file = {C:\Users\vanlissa\Zotero\storage\XDG74HWE\Lavelle - 2021 - When a Crisis Becomes an Opportunity The Role of .pdf}
}

@article{mulderPriorAdjustedDefault2014,
  title = {Prior Adjusted Default {{Bayes}} Factors for Testing (in)Equality Constrained Hypotheses},
  author = {Mulder, Joris},
  date = {2014-03-01},
  journaltitle = {Computational Statistics \& Data Analysis},
  shortjournal = {Computational Statistics \& Data Analysis},
  volume = {71},
  pages = {448--463},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2013.07.017},
  url = {http://www.sciencedirect.com/science/article/pii/S0167947313002624},
  urldate = {2020-01-15},
  abstract = {A new method is proposed for testing multiple hypotheses with equality and inequality constraints on the parameters of interest. The method is based on the fractional Bayes factor with a modification that the updated prior is centered on the boundary of the constrained parameter space under investigation. The resulting prior adjusted default Bayes factors work as an “Ockham’s razor” when testing inequality constrained hypotheses, which is not the case for the fractional Bayes factor. Two different types of prior adjusted default Bayes factors are considered. In the first type, the updated prior is based on imaginary training data. Analytical and numerical examples show that this criterion converges fastest to a true inequality constrained hypothesis. In the second type, the updated prior is based on empirical training data. This second criterion only outperforms the fractional Bayes factor in the case of small samples.},
  langid = {english},
  keywords = {(In)equality constraints,Fractional Bayes factor,Ockham’s razor,Updated prior},
  file = {C\:\\Users\\vanlissa\\Zotero\\storage\\46NG4HM3\\Mulder - 2014 - Prior adjusted default Bayes factors for testing (.pdf;C\:\\Users\\vanlissa\\Zotero\\storage\\C3UCS446\\S0167947313002624.html}
}

@article{pearlSevenToolsCausal2019,
  title = {The Seven Tools of Causal Inference, with Reflections on Machine Learning},
  author = {Pearl, Judea},
  date = {2019-02-21},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {62},
  number = {3},
  pages = {54--60},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3241036},
  url = {https://dl.acm.org/doi/10.1145/3241036},
  urldate = {2022-08-19},
  abstract = {The kind of causal inference seen in natural human thought can be "algorithmitized" to help produce human-level machine intelligence.},
  langid = {english},
  file = {C:\Users\vanlissa\Zotero\storage\ITQ9AFHS\Pearl - 2019 - The seven tools of causal inference, with reflecti.pdf}
}

@software{rcoreteamLanguageEnvironmentStatistical2022,
  title = {R: {{A Language}} and {{Environment}} for {{Statistical Computing}}},
  author = {R Core Team},
  date = {2022},
  location = {Vienna, Austria},
  url = {https://www.R-project.org},
  organization = {R Foundation for Statistical Computing}
}

@article{rileyMetaanalysisIndividualParticipant2010,
  title = {Meta-Analysis of Individual Participant Data: Rationale, Conduct, and Reporting},
  shorttitle = {Meta-Analysis of Individual Participant Data},
  author = {Riley, Richard D and Lambert, Paul C and Abo-Zaid, Ghada},
  date = {2010},
  journaltitle = {BMJ: British Medical Journal},
  volume = {340},
  number = {7745},
  eprint = {25674217},
  eprinttype = {jstor},
  pages = {521--525},
  publisher = {BMJ},
  issn = {0959-8138},
  url = {https://www.jstor.org/stable/25674217},
  urldate = {2023-03-15},
  file = {C:\Users\vanlissa\Zotero\storage\MXFN4A3G\Riley et al. - 2010 - Meta-analysis of individual participant data rati.pdf}
}

@article{rouderBayesianTestsAccepting2009,
  title = {Bayesian t Tests for Accepting and Rejecting the Null Hypothesis},
  author = {Rouder, Jeffrey N. and Speckman, Paul L. and Sun, Dongchu and Morey, Richard D. and Iverson, Geoffrey},
  date = {2009-04-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychonomic Bulletin \& Review},
  volume = {16},
  number = {2},
  pages = {225--237},
  issn = {1531-5320},
  doi = {10.3758/PBR.16.2.225},
  url = {https://doi.org/10.3758/PBR.16.2.225},
  urldate = {2024-04-09},
  abstract = {Progress in science often comes from discovering invariances in relationships among variables; these invariances often correspond to null hypotheses. As is commonly known, it is not possible to state evidence for the null hypothesis in conventional significance testing. Here we highlight a Bayes factor alternative to the conventional t test that will allow researchers to express preference for either the null hypothesis or the alternative. The Bayes factor has a natural and straightforward interpretation, is based on reasonable assumptions, and has better properties than other methods of inference that have been advocated in the psychological literature. To facilitate use of the Bayes factor, we provide an easy-to-use, Web-based program that performs the necessary calculations.},
  langid = {english},
  keywords = {Akaike Information Criterion,Marginal Likelihood,Posterior Odds,Prior Standard Deviation,Subliminal Priming},
  file = {C:\Users\vanlissa\Zotero\storage\GDRNGYWZ\Rouder et al. - 2009 - Bayesian t tests for accepting and rejecting the n.pdf}
}

@article{vanassenEndJustifiesAll2023,
  title = {The End Justifies All Means: Questionable Conversion of Different Effect Sizes to a Common Effect Size Measure},
  shorttitle = {The End Justifies All Means},
  author = {family=Assen, given=Marcel A.L.M., prefix=van, useprefix=true and Stoevenbelt, Andrea H. and family=Aert, given=Robbie C.M., prefix=van, useprefix=true},
  date = {2023-07-03},
  journaltitle = {Religion, Brain \& Behavior},
  volume = {13},
  number = {3},
  pages = {345--347},
  publisher = {Routledge},
  issn = {2153-599X},
  doi = {10.1080/2153599X.2022.2070249},
  url = {https://doi.org/10.1080/2153599X.2022.2070249},
  urldate = {2024-04-01},
  file = {C:\Users\vanlissa\Zotero\storage\7CLLDK2F\van Assen et al. - 2023 - The end justifies all means questionable conversi.pdf}
}

@article{vanleeuwenMoralityCooperationPolitics2024,
  title = {Morality as Cooperation, Politics as Conflict},
  author = {Van Leeuwen, Florian and Van Lissa, Caspar J. and Papakonstantinou, Trisevgeni and Petersen, Michael Bang and Curry, Oliver Scott},
  date = {2024-01-19},
  journaltitle = {Social Psychological Bulletin},
  shortjournal = {Soc. Psychol. Bull.},
  volume = {19},
  pages = {e10157},
  issn = {2569-653X},
  doi = {10.32872/spb.10157},
  url = {https://spb.psychopen.eu/index.php/spb/article/view/10157},
  urldate = {2024-01-31},
  abstract = {What is the relation between morality and politics? If morality is a collection of cooperative rules, and politics is conflict over which cooperative projects to pursue, then they should be correlated. We examined the relation between moral values and political orientation in samples of participants from the USA (N = 518), Denmark (N = 552), the Netherlands (N = 353), and an international online population (N = 1,337). Political conservatism was consistently related to deference values. We also found some support for the hypotheses that political orientation has distinct relations with family values and group values, and has distinct relations with fairness values and reciprocity values. However, for most hypotheses the results showed no support, largely due to poor model fit or measurement error associated with the political scales. The results suggest that improved measurement of political preferences will help understand the relation between morality and politics.},
  langid = {english},
  file = {C:\Users\vanlissa\Zotero\storage\PMN9HSQA\Van Leeuwen et al. - 2024 - Morality as cooperation, politics as conflict.pdf}
}

@article{vanlissaSelectingRelevantModerators2023a,
  title = {Selecting Relevant Moderators with {{Bayesian}} Regularized Meta-Regression},
  author = {Van Lissa, Caspar J. and family=Erp, given=Sara, prefix=van, useprefix=true and Clapper, Eli-Boaz},
  date = {2023},
  journaltitle = {Research Synthesis Methods},
  volume = {14},
  number = {2},
  pages = {301--322},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1628},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1628},
  urldate = {2024-01-31},
  abstract = {When meta-analyzing heterogeneous bodies of literature, meta-regression can be used to account for potentially relevant between-studies differences. A key challenge is that the number of candidate moderators is often high relative to the number of studies. This introduces risks of overfitting, spurious results, and model non-convergence. To overcome these challenges, we introduce Bayesian Regularized Meta-Analysis (BRMA), which selects relevant moderators from a larger set of candidates by shrinking small regression coefficients towards zero with regularizing (LASSO or horseshoe) priors. This method is suitable when there are many potential moderators, but it is not known beforehand which of them are relevant. A simulation study compared BRMA against state-of-the-art random effects meta-regression using restricted maximum likelihood (RMA). Results indicated that BRMA outperformed RMA on three metrics: BRMA had superior predictive performance, which means that the results generalized better; BRMA was better at rejecting irrelevant moderators, and worse at detecting true effects of relevant moderators, while the overall proportion of Type I and Type II errors was equivalent to RMA. BRMA regression coefficients were slightly biased towards zero (by design), but its residual heterogeneity estimates were less biased than those of RMA. BRMA performed well with as few as 20 studies, suggesting its suitability as a small sample solution. We present free open source software implementations in the R-package pema (for penalized meta-analysis) and in the stand-alone statistical program JASP. An applied example demonstrates the use of the R-package.},
  langid = {english},
  keywords = {bayesian,horseshoe,lasso,machine learning,meta-analysis,regularization},
  file = {C\:\\Users\\vanlissa\\Zotero\\storage\\J44PZV55\\Van Lissa et al. - 2023 - Selecting relevant moderators with Bayesian regula.pdf;C\:\\Users\\vanlissa\\Zotero\\storage\\EG5SS99B\\jrsm.html}
}

@incollection{vanlissaSmallSampleMetaanalyses2020,
  title = {Small Sample Meta-Analyses: {{Exploring}} Heterogeneity Using {{MetaForest}}},
  shorttitle = {Small {{Sample Size Solutions}} ({{Open Access}})},
  booktitle = {Small {{Sample Size Solutions}} ({{Open Access}}): {{A Guide}} for {{Applied Researchers}} and {{Practitioners}}},
  author = {Van Lissa, Caspar J.},
  editor = {Van De Schoot, Rens and Miočević, Milica},
  date = {2020},
  series = {European {{Association}} of {{Methodology Series}}},
  publisher = {CRC Press},
  abstract = {This unique resource provides guidelines and tools for implementing solutions to issues that arise in small sample research, illustrating statistical methods that allow researchers to apply the optimal statistical model for their research question when the sample is too small. Researchers often have},
  isbn = {978-0-367-22222-2},
  langid = {english},
  file = {C:\Users\vanlissa\Zotero\storage\AUYGHXGC\9780367222222.html}
}

@article{vanlissaTeacherCornerEvaluating2020,
  title = {Teacher’s {{Corner}}: {{Evaluating Informative Hypotheses Using}} the {{Bayes Factor}} in {{Structural Equation Models}}},
  shorttitle = {Teacher’s {{Corner}}},
  author = {Van Lissa, Caspar J. and Gu, Xin and Mulder, Joris and Rosseel, Yves and Zundert, Camiel Van and Hoijtink, Herbert},
  date = {2020-05-29},
  journaltitle = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {0},
  number = {0},
  pages = {1--10},
  publisher = {Routledge},
  issn = {1070-5511},
  doi = {10.1080/10705511.2020.1745644},
  url = {https://doi.org/10.1080/10705511.2020.1745644},
  urldate = {2020-08-18},
  abstract = {This Teacher’s Corner paper introduces Bayesian evaluation of informative hypotheses for structural equation models, using the free open-source R packages bain, for Bayesian informative hypothesis testing, and lavaan, a widely used SEM package. The introduction provides a brief non-technical explanation of informative hypotheses, the statistical underpinnings of Bayesian hypothesis evaluation, and the bain algorithm. Three tutorial examples demonstrate informative hypothesis evaluation in the context of common types of structural equation models: 1) confirmatory factor analysis, 2) latent variable regression, and 3) multiple group analysis. We discuss hypothesis formulation, the interpretation of Bayes factors and posterior model probabilities, and sensitivity analysis.},
  keywords = {Bain,bayes factor,informative hypotheses,structural equation modeling}
}

@article{viechtbauerConductingMetaanalysesMetafor2010,
  title = {Conducting Meta-Analyses in {{R}} with the Metafor Package},
  author = {Viechtbauer, Wolfgang},
  date = {2010},
  journaltitle = {Journal of Statistical Software},
  volume = {36},
  number = {3},
  pages = {1--48},
  url = {http://www.jstatsoft.org/v36/i03/},
  file = {C:\Users\vanlissa\Zotero\storage\SKIFUP3J\Viechtbauer_others_2010_Conducting meta-analyses in R with the metafor package.pdf}
}

@article{wichertsDegreesFreedomPlanning2016,
  title = {Degrees of {{Freedom}} in {{Planning}}, {{Running}}, {{Analyzing}}, and {{Reporting Psychological Studies}}: {{A Checklist}} to {{Avoid}} p-{{Hacking}}},
  shorttitle = {Degrees of {{Freedom}} in {{Planning}}, {{Running}}, {{Analyzing}}, and {{Reporting Psychological Studies}}},
  author = {Wicherts, Jelte M. and Veldkamp, Coosje L. S. and Augusteijn, Hilde E. M. and Bakker, Marjan and family=Aert, given=Robbie C. M., prefix=van, useprefix=true and family=Assen, given=Marcel A. L. M., prefix=van, useprefix=true},
  date = {2016},
  journaltitle = {Frontiers in Psychology},
  volume = {7},
  pages = {1832},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2016.01832},
  url = {https://www.frontiersin.org/article/10.3389/fpsyg.2016.01832},
  urldate = {2021-12-02},
  abstract = {The designing, collecting, analyzing, and reporting of psychological studies entail many choices that are often arbitrary. The opportunistic use of these so-called researcher degrees of freedom aimed at obtaining statistically significant results is problematic because it enhances the chances of false positive results and may inflate effect size estimates. In this review article, we present an extensive list of 34 degrees of freedom that researchers have in formulating hypotheses, and in designing, running, analyzing, and reporting of psychological research. The list can be used in research methods education, and as a checklist to assess the quality of preregistrations and to determine the potential for bias due to (arbitrary) choices in unregistered studies.},
  file = {C:\Users\vanlissa\Zotero\storage\N9SQS4FA\Wicherts et al_2016_Degrees of Freedom in Planning, Running, Analyzing, and Reporting Psychological.pdf}
}
